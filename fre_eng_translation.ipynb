{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c52f0b-aa42-4b45-97c7-fddd1f8b15d6",
   "metadata": {
    "id": "d1c52f0b-aa42-4b45-97c7-fddd1f8b15d6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence \\\n",
    "    import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MlSxM2ywnjiK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlSxM2ywnjiK",
    "outputId": "0552ec5b-90a8-4af2-fd08-6fdc6b9a6b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16c2369-45e8-4c17-9032-aab8fcf5aed5",
   "metadata": {
    "id": "d16c2369-45e8-4c17-9032-aab8fcf5aed5"
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128 #d_model \n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = 'fra.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287d13c4-22c6-4a18-8b9a-673721eab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SRC_DEST_FILE_NAME,'r') as file: \n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20c7c52-5c81-4c72-bfb3-8a2c7420885e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\\nGo.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\\nGo.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\\nGo.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\\nHi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\\nHi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\\nRun!\\tCours\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\\nRun!\\tCourez\\u202f!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\\nRun!\\tPrenez vos jambes à vos cous !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)\\nRun!\\tFile !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)\\nRun!\\tFilez !\\tCC-BY 2.0 (Fr'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d581714-86eb-4db2-951f-d785fc6a3a8a",
   "metadata": {
    "id": "1d581714-86eb-4db2-951f-d785fc6a3a8a"
   },
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024af182-c63b-4e68-a213-76b588ef4890",
   "metadata": {
    "id": "024af182-c63b-4e68-a213-76b588ef4890"
   },
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    # \"MAX_WORDS-2\" used to reserve two indices\n",
    "    # for START and STOP.\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f43a92-32dd-423b-9c44-0bdb4cd17bc3",
   "metadata": {
    "id": "94f43a92-32dd-423b-9c44-0bdb4cd17bc3"
   },
   "outputs": [],
   "source": [
    "# Read file and tokenize.\n",
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n",
    "                                       MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8369ff-6bd6-4437-b728-c25a8fbd5022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a0cc2d-28c2-4c08-bfce-4aa41524f9e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello'],\n",
       " ['hello'],\n",
       " ['hello'],\n",
       " ['hello'],\n",
       " ['hello'],\n",
       " ['hello'],\n",
       " ['i', 'see'],\n",
       " ['i', 'see'],\n",
       " ['i', 'try'],\n",
       " ['i', 'won'],\n",
       " ['i', 'won'],\n",
       " ['i', 'won'],\n",
       " ['oh', 'no'],\n",
       " ['relax'],\n",
       " ['relax'],\n",
       " ['relax'],\n",
       " ['relax'],\n",
       " ['relax'],\n",
       " ['relax'],\n",
       " ['relax']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_seq[50:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863973e-cfd0-4fcb-a532-d142974cfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the masks later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67c5847a-b669-46c5-8c15-573d1667f432",
   "metadata": {
    "id": "67c5847a-b669-46c5-8c15-573d1667f432"
   },
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_token_seq]\n",
    "src_input_data = pad_sequences(src_token_seq)\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post', maxlen\n",
    "    = len(dest_input_data[0]))\n",
    "\n",
    "# Convert to same precision as model.\n",
    "src_input_data = src_input_data.astype(np.int64)\n",
    "dest_input_data = dest_input_data.astype(np.int64)\n",
    "dest_target_data = dest_target_data.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14d55e21-9b44-4489-8042-fb9b2e9d6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_token_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42b3a803-0900-4fdf-abe6-1a59b9e8eb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 9999]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_target_token_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bdd944d-12be-41d3-8413-fb90a8acd205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9998, 27]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_input_token_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "674f4f10-44e1-4e74-b9cf-9cee30350658",
   "metadata": {
    "id": "674f4f10-44e1-4e74-b9cf-9cee30350658"
   },
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "# Create a sample of the test set that we will inspect in detail.\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_dest_input_data = test_dest_input_data[sample_indices]\n",
    "sample_dest_target_data = test_dest_target_data[sample_indices]\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(train_src_input_data),\n",
    "                         torch.from_numpy(train_dest_input_data),\n",
    "                         torch.from_numpy(train_dest_target_data))\n",
    "testset = TensorDataset(torch.from_numpy(test_src_input_data),\n",
    "                         torch.from_numpy(test_dest_input_data),\n",
    "                         torch.from_numpy(test_dest_target_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8aad7-f877-4008-bc3e-e90a4941f8ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28a8aad7-f877-4008-bc3e-e90a4941f8ab",
    "outputId": "0fae06d8-dec1-41b6-bb19-b80d487cc025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949ab2e-5fcb-4dbe-9817-b4259d78772d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2949ab2e-5fcb-4dbe-9817-b4259d78772d",
    "outputId": "815a3b91-c9f8-43cf-8284-c3bd08a98097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dest_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cf5e3e0-bcd1-4e0f-9cce-c708284e2ff7",
   "metadata": {
    "id": "4cf5e3e0-bcd1-4e0f-9cce-c708284e2ff7"
   },
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        nn.init.uniform_(self.embedding.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "\n",
    "    def forward(self,input_text):\n",
    "        return self.embedding(input_text) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310a473-433b-43d3-b5f4-e96f47448d49",
   "metadata": {
    "id": "7310a473-433b-43d3-b5f4-e96f47448d49",
    "outputId": "e49788a4-07fb-49fd-8d62-d7097aae9a45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 14, 256])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = InputEmbedding(d_model, src_vocab_size)\n",
    "sample_input_embedding = embedding(torch.tensor(sample_input_data))\n",
    "sample_input_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad7576ba-d023-4164-a32b-3e559675e16c",
   "metadata": {
    "id": "ad7576ba-d023-4164-a32b-3e559675e16c"
   },
   "outputs": [],
   "source": [
    "# Postitional Encoding Layer\n",
    "class Pos_Enc(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "\n",
    "\n",
    "    def get_angles(self,len_seq, d_model):\n",
    "\n",
    "        \"\"\"\n",
    "        Input\n",
    "        x  : input samples with shape (#samples, len_seq, len_emb = d_model)\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize the parameters\n",
    "        angles = torch.zeros((len_seq, d_model // 2))\n",
    "\n",
    "        for pos in range(len_seq):\n",
    "            for i in range(d_model//2):\n",
    "                angles[pos,i] = pos/(10000**(2*i/d_model))\n",
    "        return(angles)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This function will calculate the positional encodings for a given input x\n",
    "        Input\n",
    "        x   : input sequences with shape (#samples, len_seq, len_emb)\n",
    "\n",
    "        Output\n",
    "        pos_encoding (tensor): denoting the position of words in the sequence; shape = (1, len_seq, len_emb)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        len_seq = x.shape[1]\n",
    "        d_model = x.shape[2]\n",
    "        pos_encoding = torch.zeros((1,len_seq, d_model))\n",
    "        # Calculate the angles\n",
    "        angles = self.get_angles(len_seq,d_model)\n",
    "        # we would need a tensor of 1, len_seq, d_model) first for loop is len_seq\n",
    "        for pos in range(len_seq):\n",
    "            for i in range(angles.shape[1]):\n",
    "                pos_encoding[:,pos, 0::2] = torch.sin(angles[pos,:])\n",
    "                pos_encoding[:,pos, 1::2] = torch.cos(angles[pos,:])\n",
    "\n",
    "        # register pos_encoding as a buffer in the modul\n",
    "        self.register_buffer('pe', pos_encoding)\n",
    "        pos_encoding.requires_grad = False # not trainable\n",
    "\n",
    "        # Add the positional Encodings to the input\n",
    "        x = x + pos_encoding\n",
    "\n",
    "        # Apply Dropout and return\n",
    "        return(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33243635-052a-435f-9682-4b0d8fd3d39b",
   "metadata": {
    "id": "33243635-052a-435f-9682-4b0d8fd3d39b"
   },
   "source": [
    "#### Example: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f3201-85f8-4d20-a673-270277c15183",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "491f3201-85f8-4d20-a673-270277c15183",
    "outputId": "54cebd67-835a-4f86-aba7-b1254ba1b55d"
   },
   "outputs": [],
   "source": [
    "d_model = 16  # Size of the model\n",
    "pos_enc = Pos_Enc(0.1)\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    # Let's say we have 2 samples with a sequence length of 5\n",
    "sample_input = torch.zeros((2, 5,d_model))  # Shape: (#samples, len_seq)\n",
    "    # Get the angles\n",
    "angles = pos_enc.get_angles(5,d_model)\n",
    "\n",
    "    # Print the angles\n",
    "print(\"Computed angles:\", angles.shape) # these angles are for 5 positions\n",
    "position_enc = pos_enc(sample_input)\n",
    "print(\"Computed positional encodings:\", position_enc.shape) # these angles are for 5 positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb31f9-b883-45c6-ac82-fa77ec1c6e88",
   "metadata": {
    "id": "8cfb31f9-b883-45c6-ac82-fa77ec1c6e88"
   },
   "source": [
    "### Feed Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b09b8-54e4-415d-881a-92143b838511",
   "metadata": {
    "id": "e48b09b8-54e4-415d-881a-92143b838511"
   },
   "source": [
    "this will be a feed forward of two layers. it takes the input with the same dims as input, then runs through a d_model number of parameters. then runs it again through the len_emb neurons to return the same shape of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a76b473-cc97-47a7-9a03-5f435f7babfd",
   "metadata": {
    "id": "2a76b473-cc97-47a7-9a03-5f435f7babfd"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,d_ff,p):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.layer2 = nn.Linear(d_ff,d_model)\n",
    "        self.relu   = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        This class defines the feed forward layer of the transformer\n",
    "        - A dropout layer will be applied between the two neural layers\n",
    "        \"\"\"\n",
    "\n",
    "        return self.layer2(self.relu(self.layer1(x))) # shape: (#samples, len_seq, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70470de9-0eb3-4759-bcdf-220670de2f76",
   "metadata": {
    "id": "70470de9-0eb3-4759-bcdf-220670de2f76",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example FeedForward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a22a09-b2d9-4213-aa19-33d62293a7f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9a22a09-b2d9-4213-aa19-33d62293a7f1",
    "outputId": "38cb1806-4fbd-42c6-8e15-28d7fb77afd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example parameters\n",
    "d_model = 512  # Dimensionality of the input (hidden state size)\n",
    "d_ff = 2048    # Dimensionality of the feedforward layer\n",
    "p = 0.1        # Dropout probability\n",
    "\n",
    "# Create an instance of the FeedForward layer\n",
    "feed_forward_layer = FeedForward(d_model, d_ff, p)\n",
    "\n",
    "# Example input: shape (batch_size, len_seq, d_model)\n",
    "batch_size = 3\n",
    "len_seq = 4\n",
    "input_tensor = torch.randn(batch_size, len_seq, d_model)\n",
    "\n",
    "# Pass the input through the feedforward layer\n",
    "output_tensor = feed_forward_layer(input_tensor)\n",
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373bc93-c19d-4026-bda5-f77ee923742e",
   "metadata": {
    "id": "d373bc93-c19d-4026-bda5-f77ee923742e"
   },
   "source": [
    "### Define the maskes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7ba26-d9a1-4297-baa5-87fa07f5095f",
   "metadata": {
    "id": "8aa7ba26-d9a1-4297-baa5-87fa07f5095f"
   },
   "source": [
    "#### Look-ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3139b6b6-47db-4d81-b234-824936dddd94",
   "metadata": {
    "id": "3139b6b6-47db-4d81-b234-824936dddd94"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(dim):\n",
    "    \"\"\"\n",
    "    Creates a look-ahead mask for the decoder in transformer models.\n",
    "    At each iteration of the decoder making predictions, this function masks the proceeding words\n",
    "    to prevent the decoder from \"seeing\" future tokens.\n",
    "\n",
    "    Arguments:\n",
    "        dim -- int, the length of the sequence (or the dimensionality of the input)\n",
    "\n",
    "    Returns:\n",
    "        mask -- (1, 1, dim, dim) tensor, where the upper triangular part is 0 and the lower part is 1\n",
    "    \"\"\"\n",
    "    # Create a mask that keeps the main diagonal and all sub-diagonals, sets all super-diagonals to zero.\n",
    "    mask = torch.tril(torch.ones(dim, dim))  # Lower triangular matrix of ones\n",
    "\n",
    "    # Add an extra dimension to match the required shape (1, 1, dim, dim)\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, dim, dim)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7e8d6-27d2-4bbd-a7af-a74361aeaf99",
   "metadata": {
    "id": "e8e7e8d6-27d2-4bbd-a7af-a74361aeaf99"
   },
   "source": [
    "### Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d21bde75-e964-4f2a-b796-d7687525fca5",
   "metadata": {
    "id": "d21bde75-e964-4f2a-b796-d7687525fca5"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "\n",
    "    def __init__(self,num_heads, d_model):\n",
    "        super(MultiHeadAttn,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_params = d_model//num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
    "\n",
    "\n",
    "    def self_attention(self, q, k, v, mask):\n",
    "\n",
    "        # Calculate the dot produce of Q and K\n",
    "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
    "        # Apply the mask if givne\n",
    "        if mask is not None:\n",
    "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
    "        # Apply the Softmax to the attention weights\n",
    "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
    "        # Multiply to the value matrix\n",
    "        result = torch.matmul(attention_scores, v)\n",
    "        return(attention_scores,result)\n",
    "\n",
    "    def split_heads(self, q):\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping\n",
    "        Input of shape : (#samples, seq_len, d_model)\n",
    "\n",
    "        to\n",
    "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
    "        \"\"\"\n",
    "        samples, seq_len, _ = q.shape\n",
    "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
    "\n",
    "    def forward(self, q,k,v,mask=None):\n",
    "\n",
    "        # Define Query, Key, Value\n",
    "        Query = self.split_heads(self.W_q(q))\n",
    "        Key  = self.split_heads(self.W_k(k))\n",
    "        Value = self.split_heads(self.W_v(v))\n",
    "\n",
    "        # Run the self-attention\n",
    "        attention_scores, attn_output = self.self_attention(Query, Key,Value, mask)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        batch_size, _,seq_len, _= attn_output.shape\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
    "\n",
    "        # Apply through the last linear layer\n",
    "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
    "\n",
    "        return result_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdc650-9c9d-4dc5-8f39-212382b4250a",
   "metadata": {
    "id": "8bfdc650-9c9d-4dc5-8f39-212382b4250a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1894ba-6d84-4a3a-b3c3-eb5341f1889b",
   "metadata": {
    "id": "2a1894ba-6d84-4a3a-b3c3-eb5341f1889b",
    "outputId": "b26a817e-d8f9-4b0f-d943-01b6e4812c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 14, 256])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 64\n",
    "seq_length = 10\n",
    "d_model = 256  # Embedding size\n",
    "num_heads = 8\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Instantiate the MultiHeadAttn class\n",
    "multi_head_attention = MultiHeadAttn(num_heads, d_model, dropout_prob)\n",
    "\n",
    "# Create random input tensors for queries, keys, and values\n",
    "q = torch.rand(batch_size, seq_length, d_model)  # Shape: (64, 10, 256) #(#samples, len_seq, #len_emb)\n",
    "k = torch.rand(batch_size, seq_length, d_model)  # Shape: (64, 10, 256)\n",
    "v = torch.rand(batch_size, seq_length, d_model)  # Shape: (64, 10, 256)\n",
    "\n",
    "# Optional masking (for example, padding mask)\n",
    "masking = create_padding_mask(torch.tensor(sample_input_data),num_heads)\n",
    "# Forward pass\n",
    "dotqk = multi_head_attention(sample_input_embedding, sample_input_embedding, sample_input_embedding, masking)\n",
    "\n",
    "# Check the output shape\n",
    "print(output.shape)  # Should print: torch.Size([64, 10, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d060ad-57f8-4327-a519-169cb27fedb5",
   "metadata": {
    "id": "73d060ad-57f8-4327-a519-169cb27fedb5",
    "outputId": "feba446f-810d-4a67-9478-79a1817a50c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1190,  0.1190,  0.1190,  0.1190,  0.1190,  0.1190,  0.1190,  0.1190,\n",
       "        -0.0147,  0.0795,  0.0811,  0.7231, -0.5528,  0.2886],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotqk[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471170b3-e977-495d-80f2-2431b97b6d45",
   "metadata": {
    "id": "471170b3-e977-495d-80f2-2431b97b6d45",
    "outputId": "c71c1618-1756-4fce-a030-5ecf13c099ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -0.0000e+00, -0.0000e+00,\n",
       "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking[1][0][0] * -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd1f7a-0f99-4ab3-b0d8-59e2e5f93cf9",
   "metadata": {
    "id": "f0bd1f7a-0f99-4ab3-b0d8-59e2e5f93cf9",
    "outputId": "620cc84b-4093-4b82-a012-d241a6cc61b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1383,\n",
       "        0.1520, 0.1522, 0.2893, 0.0808, 0.1873], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(dotqk[1][0][0] + masking[1][0][0] * -1e9, dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee73e2d-0c73-415f-9c17-208e41e738a5",
   "metadata": {
    "id": "3ee73e2d-0c73-415f-9c17-208e41e738a5"
   },
   "source": [
    "But if we use a dropout, then the attention scores might be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc55aea-008b-4369-a586-f6959f70a120",
   "metadata": {
    "id": "9bc55aea-008b-4369-a586-f6959f70a120",
    "outputId": "08b92ca5-6ead-423f-ad0d-20c7f9a9559a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0.0000, 0.1302, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1363,\n",
      "        0.1329, 0.1186, 0.1464, 0.1107, 0.2250], grad_fn=<SelectBackward0>)\n",
      "[   0    0    0    0    0    0    0    0 8070    9 1728    7 1099  494]\n"
     ]
    }
   ],
   "source": [
    "print(masking[1][0])\n",
    "print(attention_scores[1][0][0])\n",
    "print(sample_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09b3d0e0-4a40-4535-ab08-01fa1d4a566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0   14    8   32    4   40 1569]\n"
     ]
    }
   ],
   "source": [
    "print(sample_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e452ae9-c887-4b11-a718-1b9203131ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = sample_input_data[1] == 0 \n",
    "mask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d15bd-7002-47c8-9cc8-63e1755c36f1",
   "metadata": {
    "id": "610d15bd-7002-47c8-9cc8-63e1755c36f1"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96983bd8-de8d-4e6b-94cd-08e319209b4e",
   "metadata": {
    "id": "96983bd8-de8d-4e6b-94cd-08e319209b4e"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, d_model,p, d_ff):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        self.mha       = MultiHeadAttn(num_heads, d_model)\n",
    "        self.ffn       = FeedForward(d_model,d_ff,p)\n",
    "\n",
    "        self.dropout  = nn.Dropout(p)\n",
    "\n",
    "        self.norm1     = nn.LayerNorm(d_model)\n",
    "        self.norm2     = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, pad_mask) ->torch.FloatTensor:\n",
    "\n",
    "        # Run through a mha and dropout\n",
    "        attention_res = self.dropout(self.mha(x,x,x, pad_mask))\n",
    "        # Add and Normalize\n",
    "        normalize_attn_scores = self.norm1(attention_res + x)\n",
    "        # Run through the feed forward NN and dropout\n",
    "        res_ffn = self.dropout(self.ffn(normalize_attn_scores))\n",
    "        # Add and Normalize\n",
    "        x = self.norm2(res_ffn + normalize_attn_scores)\n",
    "        \n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158fcc2f-26a8-4aa3-83eb-2999ab9dc09e",
   "metadata": {
    "id": "158fcc2f-26a8-4aa3-83eb-2999ab9dc09e"
   },
   "source": [
    "#### Example Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6a730-9674-4315-ad1a-21b522cf14e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "36b6a730-9674-4315-ad1a-21b522cf14e7",
    "outputId": "c3af8d18-72a9-4d17-a49f-0f91a79dcc8e"
   },
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "seq_length = 14\n",
    "vocab_size = MAX_WORDS\n",
    "d_model = 60\n",
    "num_heads =10\n",
    "d_ff = 60\n",
    "num_iter = 6\n",
    "dropout_rate = 0.1\n",
    "pad_mask = None  # You can specify a padding mask if needed\n",
    "\n",
    "# Instantiate the Encoder\n",
    "encoder = Encoder(heads=num_heads, d_model=d_model, p=dropout_rate, d_ff=d_ff)\n",
    "\n",
    "# Forward pass through the Encoder\n",
    "output_encoder = encoder(torch.tensor(sample_input_data), pad_mask = None)\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6582c82-eb23-426c-99cb-54b55bcfb6e0",
   "metadata": {
    "id": "b6582c82-eb23-426c-99cb-54b55bcfb6e0"
   },
   "source": [
    "### Define the Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c50b3f-65aa-4f92-ab48-6bbf2f6b8877",
   "metadata": {
    "id": "57c50b3f-65aa-4f92-ab48-6bbf2f6b8877"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, p, d_ff, d_model):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.mha1 = MultiHeadAttn(num_heads, d_model)\n",
    "        self.mha2 = MultiHeadAttn(num_heads, d_model)\n",
    "\n",
    "        self.ffn = FeedForward(d_model,d_ff,p)\n",
    "\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, padding_mask, look_ahead_mask):\n",
    "\n",
    "        # Run the input decdoer through a MHA and add dropout\n",
    "        res_attn1 = self.drop(self.mha1(x,x,x,look_ahead_mask))\n",
    "        # Add and Normalize\n",
    "        normalized_res_attn1 = self.norm1(res_attn1 + x)\n",
    "        # Run through the second MHA and add dropout\n",
    "        res_attn2 = self.drop(self.mha2(normalized_res_attn1, enc_output, enc_output, padding_mask))\n",
    "        # Add and Normalize\n",
    "        normalized_attn2 = self.norm2(res_attn2 + normalized_res_attn1)\n",
    "        # Run throught the Feed Forward NN and apply dropout\n",
    "        res_ffn = self.drop(self.ffn(normalized_attn2))\n",
    "        # Add and Normalize\n",
    "        normalized_res_ffn = self.norm3(res_ffn + normalized_attn2)\n",
    "        x = normalized_res_ffn\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7c72f-37e3-4a9e-9c7c-2579f6a3708c",
   "metadata": {
    "id": "12e7c72f-37e3-4a9e-9c7c-2579f6a3708c"
   },
   "source": [
    "#### Example Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d1685-fd96-4bff-b559-c74982ab2234",
   "metadata": {
    "id": "8a7d1685-fd96-4bff-b559-c74982ab2234",
    "outputId": "aca2eb36-ebaa-4533-c7ed-f4c83dcb8aa3"
   },
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "seq_length = 14\n",
    "vocab_size = MAX_WORDS\n",
    "d_model = 60\n",
    "num_heads =10\n",
    "d_ff = 60\n",
    "num_iter = 6\n",
    "dropout_rate = 0.1\n",
    "pad_dec_mask = None  # You can specify a padding mask if needed\n",
    "look_ahead_mask1 = create_look_ahead_mask(9)\n",
    "# Instantiate the Encoder\n",
    "decoder = Decoder(num_heads=num_heads, p=dropout_rate,  d_ff=d_ff, d_model=d_model)\n",
    "\n",
    "# Forward pass through the Encoder\n",
    "output_decoder = decoder(torch.tensor(sample_dest_input_data), output_encoder, pad_dec_mask = None, look_ahead_mask=look_ahead_mask)\n",
    "output_decoder.shape # what is this? these are probability? for 9 words of the length of the output which is not more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce2098-07bf-44cf-9253-97d9564164c1",
   "metadata": {
    "id": "42ce2098-07bf-44cf-9253-97d9564164c1"
   },
   "source": [
    "### Define the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81af156-cfd1-4079-a283-9bef6c39ecd6",
   "metadata": {
    "id": "f81af156-cfd1-4079-a283-9bef6c39ecd6"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, p, d_ff, d_model, num_iter,src_vocab_size, tgt_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.Embedding_enc = InputEmbedding(d_model, src_vocab_size)\n",
    "        self.Embedding_dec = InputEmbedding(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.pos_encoder = Pos_Enc()\n",
    "        self.pos_decoder = Pos_Enc()\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([Encoder(num_heads, d_model,p, d_ff) for _ in range(num_iter)])\n",
    "        self.decoder_layers = nn.ModuleList([Decoder(num_heads, p, d_ff, d_model) for _ in range(num_iter)])\n",
    "\n",
    "        self.linear  = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = create_look_ahead_mask(seq_length).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self,input_enc, input_dec):\n",
    "\n",
    "        src_mask, tgt_mask = self.generate_mask(input_enc, input_dec)\n",
    "\n",
    "        ### Encoder Layers\n",
    "        # Embedding layer\n",
    "        input_enc_emb = self.Embedding_enc(input_enc)\n",
    "        # Positional encoding\n",
    "        input_enc_final = self.pos_encoder(input_enc_emb)\n",
    "        input_enc_final *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        #Encoder layers\n",
    "        # Initially assume enc_output is the enc_input\n",
    "        enc_output = input_enc_final\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        ### Decoder Layers\n",
    "        # Embedding layer\n",
    "        input_dec_emb = self.Embedding_dec(input_dec)\n",
    "        # Add positional encodings\n",
    "        input_dec_final = self.pos_decoder(input_dec_emb)\n",
    "        input_dec_final *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # Decoder layers\n",
    "        dec_output = input_dec_final\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        ### Run thourgh the final layer\n",
    "        final_output = self.linear(dec_output)\n",
    "\n",
    "        return(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df999bc0-04b0-4c90-9638-775c5fa30bc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df999bc0-04b0-4c90-9638-775c5fa30bc3",
    "outputId": "543346bb-4a1f-483e-f81a-b40eaa2f9b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5837584\n"
     ]
    }
   ],
   "source": [
    "num_heads = 8\n",
    "p = 0.1\n",
    "d_ff = LAYER_SIZE\n",
    "d_model= EMBEDDING_WIDTH\n",
    "num_iter= 6\n",
    "src_vocab_size = MAX_WORDS\n",
    "tgt_vocab_size = MAX_WORDS\n",
    "\n",
    "\n",
    "#look_ahead_mask  = create_look_ahead_mask(9)\n",
    "transformer = Transformer(num_heads, p, d_ff, d_model, num_iter,src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# Loss functions and optimizer.\n",
    "optimizer = torch.optim.RMSprop(transformer.parameters(), lr=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe3957d-2501-4966-ae05-61ebeec23710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe3957d-2501-4966-ae05-61ebeec23710",
    "outputId": "965cd0ec-9e21-4bff-f1f9-7992ebec797f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 9, 10000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_transformer = transformer(torch.tensor(sample_input_data), torch.tensor(sample_dest_input_data))\n",
    "output_transformer.shape #each row is a probability distribution showing the most likely word to be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a39d3c-439d-4a12-80e4-9343d7272d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the train and eval (),t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "250ee30c-ae90-4dcd-84d3-e97433da5f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "250ee30c-ae90-4dcd-84d3-e97433da5f40",
    "outputId": "131561ac-f504-4484-f705-c311b5fb80e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 loss: 0.0008 - acc: 0.9354 - val_loss: 0.5884 - val_acc: 0.8972\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'will', 'be', 'content', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'back', 'in', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'seemed', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/20 loss: 0.0006 - acc: 0.9379 - val_loss: 0.5856 - val_acc: 0.8979\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'satisfied', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"that's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'that', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'seemed', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/20 loss: 0.0007 - acc: 0.9398 - val_loss: 0.5816 - val_acc: 0.8976\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'satisfied', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"that's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/20 loss: 0.0006 - acc: 0.9412 - val_loss: 0.5786 - val_acc: 0.8987\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'satisfied', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/20 loss: 0.0006 - acc: 0.9426 - val_loss: 0.5828 - val_acc: 0.8981\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'satisfied', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'that', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'seemed', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/20 loss: 0.0006 - acc: 0.9450 - val_loss: 0.5794 - val_acc: 0.8985\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'satisfied', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'that', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'seemed', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7/20 loss: 0.0007 - acc: 0.9470 - val_loss: 0.5822 - val_acc: 0.8988\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'will', 'be', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8/20 loss: 0.0006 - acc: 0.9479 - val_loss: 0.5808 - val_acc: 0.9000\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'off', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9/20 loss: 0.0005 - acc: 0.9498 - val_loss: 0.5817 - val_acc: 0.9004\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'bad', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'this', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10/20 loss: 0.0006 - acc: 0.9513 - val_loss: 0.5798 - val_acc: 0.8998\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'a', 'le', 'cancer']\n",
      "Target Sent:\n",
      "['tom', 'has', 'cancer', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'has', 'cancer', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'peux', 'tu', \"t'en\", 'occuper']\n",
      "Target Sent:\n",
      "['can', 'you', 'handle', 'it', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'can', 'you', 'handle', 'it', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'se', 'souvient', 'de', 'toi']\n",
      "Target Sent:\n",
      "['tom', 'remembers', 'you', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'remembers', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'grave\\u202f']\n",
      "Target Sent:\n",
      "['is', 'it', 'important', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'is', 'it', 'serious', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'étourdi']\n",
      "Target Sent:\n",
      "[\"i'm\", 'forgetful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'dizzy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'une', 'pêche']\n",
      "Target Sent:\n",
      "['this', 'is', 'a', 'peach', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"it's\", 'a', 'fishing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'occupée']\n",
      "Target Sent:\n",
      "[\"i'm\", 'busy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'busy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'veux', 'y', 'aller']\n",
      "Target Sent:\n",
      "['i', 'want', 'to', 'go', 'there', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'want', 'to', 'go', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', 'semblaient', 'satisfaites']\n",
      "Target Sent:\n",
      "['they', 'seemed', 'content', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'they', 'drove', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'bête']\n",
      "Target Sent:\n",
      "[\"it's\", 'not', 'stupid', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"that's\", 'not', 'stupid', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'êtes', 'fascinant']\n",
      "Target Sent:\n",
      "[\"you're\", 'fascinating', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"you're\", 'fascinating', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'ici', 'pour', \"t'aider\"]\n",
      "Target Sent:\n",
      "[\"i'm\", 'here', 'to', 'help', 'you', 'STOP', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', \"i'm\", 'here', 'to', 'help', 'you', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mets', 'ça', 'par', 'écrit']\n",
      "Target Sent:\n",
      "['put', 'that', 'in', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'put', 'that', 'in', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'ai\", 'du', 'sable', 'dans', \"l'œil\"]\n",
      "Target Sent:\n",
      "['i', 'got', 'sand', 'in', 'my', 'eye', 'STOP', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'i', 'have', 'an', 'eye', 'on', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'dépêche', 'toi', 'je', 'te', 'prie']\n",
      "Target Sent:\n",
      "['hurry', 'up', 'please', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'please', 'hurry', 'up', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'avait', \"l'air\", 'heureux']\n",
      "Target Sent:\n",
      "['tom', 'looked', 'happy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'looked', 'happy', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'ne', 'sait', 'rien']\n",
      "Target Sent:\n",
      "['tom', 'knows', 'nothing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'knows', 'nothing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'soyez', 'prudent']\n",
      "Target Sent:\n",
      "['be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'be', 'careful', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'commence', 'à', 'écrire']\n",
      "Target Sent:\n",
      "['start', 'writing', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'start', 'writing', 'STOP']\n",
      "\n",
      "\n",
      "\n",
      "Source Sent:\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tom', 'est', 'un', 'garçon', 'brillant']\n",
      "Target Sent:\n",
      "[\"tom's\", 'a', 'smart', 'guy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Predicted Sent:\n",
      "['START', 'tom', 'is', 'a', 'bright', 'boy', 'STOP']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for i in range(10):\n",
    "    transformer.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_batches = 0\n",
    "    train_elems = 0\n",
    "\n",
    "    for src_input, tgt_input, tgt_output in trainloader:\n",
    "\n",
    "        # Zero the parameter gradiets\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run a prediction on the data\n",
    "        output_transformer = transformer(src_input, tgt_input)\n",
    "        # calculate the loss\n",
    "        loss = loss_function(output_transformer.view(-1, MAX_WORDS), tgt_output.view(-1)) #shape: (batch_size * len_seq,MAX_WORDS) vs (batch_size*len_seq)\n",
    "        # Accumulate metrics\n",
    "        _, indices = torch.max(output_transformer.data, 2)\n",
    "        train_correct += (indices == tgt_output).sum().item()# number of indecies being predicted correctly \n",
    "        train_elems += indices.numel() #number of elements predicted \n",
    "        train_batches +=  1 #number of batches trained \n",
    "        train_loss += loss.item()\n",
    "        # Backward pass and update.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_elems\n",
    "\n",
    "    # Evaluate the model on the test dataset.\n",
    "    transformer.eval() # Set model in inference mode.\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_batches = 0\n",
    "    test_elems = 0\n",
    "    for src_inputs, dest_inputs, dest_targets in testloader:\n",
    "\n",
    "        # Make a prediction\n",
    "        outputs = transformer(src_inputs, dest_inputs)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        _, indices = torch.max(outputs, 2)\n",
    "        test_correct += (indices == dest_targets).sum().item()\n",
    "        test_elems += indices.numel()\n",
    "        test_batches +=  1\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss = test_loss / test_batches\n",
    "    test_acc = test_correct / test_elems\n",
    "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')\n",
    "\n",
    "    # Loop through a each sample in the sample dataset\n",
    "    for sample_enc_input, sample_dec_target in zip(sample_input_data,\n",
    "                                             sample_dest_target_data):\n",
    "        # Convert to torch tensor\n",
    "        sample_enc_input = torch.tensor(np.reshape(sample_enc_input,(1,-1)))\n",
    "        sample_dec_target = torch.tensor(np.reshape(sample_dec_target,(1,-1)))\n",
    "\n",
    "        # Initialize the input decoder\n",
    "        input_dec = np.reshape(START_INDEX,(1,-1))\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        # we have no words predicted. we loop to predict 60 words unless the stop word is predicted.\n",
    "        for j in range(MAX_LENGTH):\n",
    "\n",
    "            # Predict next word\n",
    "            outputs = transformer(sample_enc_input, torch.tensor(input_dec))\n",
    "\n",
    "            # Find the index of the most probable word and apprend it\n",
    "            next_index = outputs[:,-1,:].argmax().numpy()\n",
    "            input_dec = np.reshape(np.append(input_dec, next_index),(1,-1))\n",
    "            # Stop the algorithm if the STOP token is predicted\n",
    "            if next_index == STOP_INDEX:\n",
    "                break\n",
    "\n",
    "        # Convert the predicted indexes to words\n",
    "        print(f\"Source Sent:\")\n",
    "        tokens_to_words(src_tokenizer, sample_enc_input.tolist()[0])\n",
    "        print(f\"Target Sent:\")\n",
    "        tokens_to_words(dest_tokenizer, sample_dec_target.tolist()[0])\n",
    "        print(f\"Predicted Sent:\")\n",
    "        tokens_to_words(dest_tokenizer, input_dec.tolist()[0])\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ol6qHPX-wopc",
   "metadata": {
    "id": "ol6qHPX-wopc"
   },
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"transformer_model.pth\")#after 19 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6351a90-7521-4fbc-a6a6-8fba5e207454",
   "metadata": {
    "id": "a6351a90-7521-4fbc-a6a6-8fba5e207454"
   },
   "source": [
    "work on the custom schedule learning rate and the line where we collapse the outputs. check the code for paddings in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GSKO1M7BwnNg",
   "metadata": {
    "id": "GSKO1M7BwnNg"
   },
   "outputs": [],
   "source": [
    "#examine how the accuracy works"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

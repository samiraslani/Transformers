{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91892c6-90fa-48e6-9a7b-2e173e4b0a5c",
   "metadata": {},
   "source": [
    "The objective of this project is to: \n",
    "* Create positional encodings to capture sequential relationships in data\n",
    "* Calculate scaled dot-product self-attention with word embeddings\n",
    "* Implement masked multi-head attention\n",
    "* Build and train a Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf32da35-c73d-4fd9-ac13-b9611a826739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages: \n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization, Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import  reshape, shape, transpose\n",
    "\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24993869-fed5-49e8-bbb3-7c2974df863c",
   "metadata": {},
   "source": [
    "## Creating the word embeddings based on GloVe embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72957b17-242f-42a0-bdff-f77ef8329a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the input with Glove Word Embeddings: \n",
    "def gvec_input(x,m,e): \n",
    "    \"\"\"\n",
    "    \n",
    "    This function takes any input, x, and returns a glove vector based on the \n",
    "    words introduced in the vocabulary (400,000 words). This function returns k vectors where k is the number of words in the \n",
    "    sentence. Every vector corresponds to a word in the dictionary and each entry will describe a feature of the word. \n",
    "    \n",
    "    inputs: \n",
    "    \n",
    "    x (string) : a statement from customers. \n",
    "    m (int)    : size of the sequence \n",
    "    e (int)    : size of the embeddings \n",
    "    outputs: \n",
    "    v (m,n)    : where m is the number of words in the sentence and n = 50 is the number of total features describing a word. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    gv = np.zeros((n,m, e))\n",
    "    \n",
    "    for i in range(0, n): #looping over each comment \n",
    "        txt = x[i] #select the ith comment  \n",
    "        txt = (txt[:m] if len(txt) > m else txt + ['<pad>'] * (m - len(txt))) #shorten or add extra padding\n",
    "        for l in range(m): #looping over each word \n",
    "            \n",
    "            # add the embedding of all ones for pads\n",
    "            if txt[l] == \"<pad>\": \n",
    "                gv[i,l,:] = np.ones(e) \n",
    "                \n",
    "            # if a word is not is the list of Glove embeddings, then assign an array which is the average of all embeddings:    \n",
    "            elif txt[l] not in words: \n",
    "                gv[i,l,:] = np.mean(vectors, axis = 0)\n",
    "            # add the word embeddings: \n",
    "            else: \n",
    "                gv[i,l,:] = embeddings_dict[txt[l]]\n",
    "    return(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84cded1-8bc4-4eec-9f9b-cccb5b98983b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The length of the embeddings: \u001b[39;00m\n\u001b[1;32m      4\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m X_trainmod \u001b[38;5;241m=\u001b[39m gvec_input(X_train,m,e) \u001b[38;5;66;03m#X_train is training dataset modified (edited and tokenized);\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                                      \u001b[38;5;66;03m# shape of X_trainmod (#samples, len_seq, len_emb) \u001b[39;00m\n\u001b[1;32m      8\u001b[0m X_testmod \u001b[38;5;241m=\u001b[39m gvec_input(X_test,m,e)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Limit the length of the sequence: \n",
    "m = 30 \n",
    "# The length of the embeddings: \n",
    "e = 50\n",
    "X_trainmod = gvec_input(X_train,m,e) #X_train is training dataset modified (edited and tokenized);\n",
    "                                     # shape of X_trainmod (#samples, len_seq, len_emb) \n",
    "\n",
    "X_testmod = gvec_input(X_test,m,e)   #X_test will be the testing dataset modified (edited and tokenized) \n",
    "                                     # shape of X_testmod (#samples, len_seq, len_emb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc2847-c7c2-43a5-8dfe-73865ee03fe2",
   "metadata": {},
   "source": [
    "## Creating the positional encodings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637d96b0-8376-4518-a333-fcb383b7670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the angles for positional embeddings: \n",
    "\n",
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get i from dimension span k\n",
    "    i = k//2\n",
    "    # Calculate the angles using pos, i and d\n",
    "    angles = pos/ (10000)**(2*i/d)\n",
    "\n",
    "    \n",
    "    return angles\n",
    "    \n",
    "def pos_emb(len_seq,len_emb): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates the positional embeddings for all the words in the sequence based on: \n",
    "    \n",
    "    Input: \n",
    "    len_seq (int) : The length of the sequences inputed into the model. \n",
    "    len_emb (int) : The length of the word embeddings for every word in the sequence. \n",
    "\n",
    "    Note: the size of the positional encoding and the word embeddings must match in order to add them in the next step. \n",
    "\n",
    "    Output: \n",
    "    res (np.array(len_seq, len_emb)) : ith row of this matrix represents the positional encodings for the ith position in the sequence. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    len_i = int(len_emb/2)\n",
    "\n",
    "    # Initialize the matrix to save positional encodings: \n",
    "    res = np.zeros((len_seq,len_emb))\n",
    "    angles = np.zeros((len_seq,len_emb))\n",
    "    \n",
    "    #for each position in the sequence \n",
    "    for pos in range(len_seq): \n",
    "        \n",
    "        #calculate the angles: \n",
    "        for i in range(len_i): \n",
    "            angles[pos,2*i] = pos/(10000**(2*i/len_emb))\n",
    "            angles[pos, 2*i +1] = pos/(10000**(2*i/len_emb)) \n",
    "        \n",
    "        # Calculate the entries corresponding to each position \n",
    "        #for j in range(len_i): \n",
    "        res[pos, 0::2] = np.sin(angles[pos,0::2])\n",
    "        res[pos,1::2] = np.cos(angles[pos,0::2])\n",
    "            \n",
    "    return(tf.cast(res.reshape(1,len_seq,len_emb), dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c529243c-143a-4c19-b33e-09f20339c327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 30, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the positional embeddings: \n",
    "position_enc = pos_emb(X_trainmod.shape[1],X_trainmod.shape[2])\n",
    "position_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "403a6f0c-6c2e-4f39-b203-df7d9ba0c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 30, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([375, 30, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the positional encoding to the word embeddings: \n",
    "X_trainmod = X_trainmod + position_enc \n",
    "print(X_trainmod.shape)\n",
    "\n",
    "X_testmod = X_testmod + position_enc \n",
    "X_testmod.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d82d35-b01d-4f64-b60e-742859cd1516",
   "metadata": {},
   "source": [
    "## Defining the mask functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2610eef7-b508-4824-986e-26fa7a3a42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(tensor): \n",
    "    \"\"\"\n",
    "    this function will encode the padded sequences as -1e-9 so that when they are run through a Softmax, the value will be equal to zero. \n",
    "    \"\"\"\n",
    "    # Identify rows where all elements are zero\n",
    "    is_zero_row = tf.reduce_all(tf.equal(tensor, 0), axis=1)\n",
    "\n",
    "    # Expand is_zero_row to match the shape of tensor\n",
    "    is_zero_row_expanded = tf.expand_dims(is_zero_row, axis=-1)\n",
    "\n",
    "    # Replace zeros with -1e-9 where the row is all zeros\n",
    "    result_tensor = tf.where(is_zero_row_expanded, tf.constant(-1e-9, dtype=tf.float64), tensor)\n",
    "    return(result_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824583e-f0dd-499d-a464-3f5a424ba195",
   "metadata": {},
   "source": [
    "## Define the self attention: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6376dbc-8cdc-4021-864d-00d204ab466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(q,k,v, masking):\n",
    "    \"\"\"\n",
    "    this function calculates a self_attention mechanism \n",
    "    res are the final attention scores. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Perform matrix multiplication on the last two dimensions\n",
    "    dotqk = tf.matmul(q, k, transpose_b = True)\n",
    "\n",
    "    dim_k = tf.cast(k.shape[-1],tf.float32)\n",
    "    normalized_dotqk = dotqk/tf.math.sqrt(dim_k)\n",
    "    \n",
    "    #then add the masking if masking if given\" \n",
    "    if masking is not None: \n",
    "        normalized_dotqk += (1 - masking)* (-1e9)\n",
    "    \n",
    "    attention_scores =  tf.nn.softmax(tf.cast(normalized_dotqk, dtype=tf.float32),axis = -1)\n",
    "    res = tf.matmul(attention_scores,v) \n",
    "    \n",
    "    return(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc50a9-15bc-4a67-8f91-67165834ea85",
   "metadata": {},
   "source": [
    "## Defining the feed forward neural network: \n",
    "This will be used as a part of the encoder and decoder structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "645c8842-8c75-4d7d-84f6-7cc6dc1a2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullFeedForward(n_1, emb_size):#the model must return vectors of the same size as the embeddings of the input so can be combined with decoder\n",
    "    model = Sequential([\n",
    "    Dense(n_1, activation='tanh', name=\"dense1\"), #relu? (#samples, len_seq, n_1)\n",
    "    Dense(emb_size, activation='tanh', name=\"dense2\")# linear? (#samples, len_seq, emb_size)\n",
    "])\n",
    "    return(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1c8d12-2822-4f45-b2ff-0ffa7fa5e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reshape_tensor which will be later on used for the Multi-head attention: \n",
    "\n",
    "def reshape_tensor(q_matrix, heads, pre_attention): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    #pre_attention, we'll need to reform into 4d \n",
    "    if pre_attention:\n",
    "\n",
    "        dense_qre = reshape(q_matrix, (shape(q_matrix)[0], shape(q_matrix)[1], heads, -1))\n",
    "        dense_qre = transpose(dense_qre, ([0, 2, 3, 1]))\n",
    "        \n",
    "        \n",
    "    #post_attention, we'll need to revert back to 3d: \n",
    "    else: \n",
    "        q_matrix_transpose = transpose(q_matrix, ([0,3,1,2]))\n",
    "        dense_qre = reshape(q_matrix_transpose, (shape(q_matrix_transpose)[0], shape(q_matrix_transpose)[1], -1)) \n",
    "        \n",
    "        \n",
    "    return(dense_qre)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8258207-265a-4ebe-a61c-b4de933b4ae4",
   "metadata": {},
   "source": [
    "## Define the class for multi-head attention: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1380463-0062-4718-b56c-dd21b0823aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer): \n",
    "    \n",
    "    def __init__(self, dim_kv, dim_q, len_emb, heads, **kwargs):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(**kwargs) \n",
    "        self.dim_k = self.dim_v = dim_kv\n",
    "        self.dim_q = dim_q\n",
    "        self.heads = heads\n",
    "        self.d_model = len_emb\n",
    "\n",
    "    \n",
    "    def call(self,q,k,v, masking = None, **kwargs): #by passing self, you passed all the attributes you've defined above. \n",
    "       \n",
    "        # Define the query, key, and value matrices: \n",
    "        dense_q = Dense(units = self.dim_q)(q) # shape = (#samples, len_seq, dim_q)\n",
    "        dense_k = Dense(units = self.dim_k)(k) # shape = (#samples, len_seq, dim_k) \n",
    "        dense_v = Dense(units = self.dim_v)(v) # shape = (#samples, len_seq, dim_v) \n",
    "        \n",
    "        # Reshape: \n",
    "        dense_qre = reshape_tensor(dense_q, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        dense_kre = reshape_tensor(dense_k, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_k/heads, len_seq)\n",
    "        dense_vre = reshape_tensor(dense_v, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_v/heads, len_seq) \n",
    "        \n",
    "        # Calculate the attention scores: \n",
    "        attention_scores = self_attention(dense_qre, dense_kre,dense_vre, masking) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        \n",
    "        # Revert the shape:\n",
    "        attention_with_v = reshape_tensor(attention_scores, self.heads, pre_attention = False) #shape = (#samples, len_seq, dim_q)\n",
    "        \n",
    "        # Run through another dense and add to the initial x: \n",
    "        res = Dense(units = self.d_model)(attention_with_v)  # shape = (#samples, len_seq, d_model) \n",
    "        \n",
    "        return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ce61987-9ca6-4042-ae9d-846c54c1199b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1125, 30, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it works: \n",
    "dim_kv = 30 \n",
    "dim_q = 20 \n",
    "len_emb = 50\n",
    "heads = 2 \n",
    "\n",
    "masking = None\n",
    "\n",
    "function = MultiHeadAttention(dim_kv, dim_q, len_emb, heads)\n",
    "function(X_trainmod, X_trainmod,X_trainmod, masking = None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d98f9a-346a-4605-9fbd-32df11e4da5a",
   "metadata": {},
   "source": [
    "## Define the Encoder layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40656c8b-23c9-403e-9bc8-566f49c50af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    \n",
    "    def __init__(self, dim_kv, dim_q, heads, fnn_neurons, len_emb, iter, drop_rate):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        self.mha     = MultiHeadAttention(dim_kv, dim_q, len_emb, heads)\n",
    "        self.norm    = LayerNormalization(epsilon = 1e-6)\n",
    "        self.drop    = Dropout(rate = drop_rate)\n",
    "        self.fnn     = FullFeedForward(fnn_neurons, len_emb)\n",
    "        self.iter    = iter\n",
    "\n",
    "        \n",
    "    def call(self,x,training, masking): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(self.iter): \n",
    "\n",
    "            # Add dropout layer: \n",
    "            drop_x = self.drop(x, training = training) \n",
    "            \n",
    "            # Calculate the attention scores: \n",
    "            mha_scores = self.mha(drop_x, drop_x, drop_x, masking = masking)\n",
    "        \n",
    "            # Add dropout and normalize: \n",
    "            dropout_1 = self.drop(mha_scores, training = training)\n",
    "            norm_1  = self.norm(dropout_1 + x )\n",
    "        \n",
    "            #Run through a fully connected neural network: \n",
    "            fnn_output = self.fnn(norm_1) \n",
    "            \n",
    "            # Add dropout: \n",
    "            dropout_2 = self.drop(fnn_output, training = training)\n",
    "        \n",
    "            # Normalize: \n",
    "            x = self.norm(dropout_2 + norm_1)\n",
    "            \n",
    "        return x\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "289dfe62-5023-400c-9778-08acedc534a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1125, 30, 50])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it works: \n",
    "dim_kv = 30 \n",
    "dim_q = 20 \n",
    "len_emb = 50\n",
    "heads = 2 \n",
    "masking = None \n",
    "fnn_neurons = 20\n",
    "drop_rate = 0.1\n",
    "function = Encoder(dim_kv, dim_q, heads, fnn_neurons, len_emb, drop_rate,10)\n",
    "output_encoder = function(X_trainmod, masking = None)\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e5f21-f3b1-4018-849d-8288983c6a87",
   "metadata": {},
   "source": [
    "## Define the Decoder layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c75cbb94-77cc-4907-a737-fd8d5d907a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self, len_emb, dim_kv, dim_q, heads, \n",
    "                dd_model, iter, \n",
    "                drop_rate = 0.1, epsilon = 1e-6):  #dd_model is the number of neurons in the last layer of decoder (dense with softmax) \n",
    "        super(Decoder, self).__init__()\n",
    "        self.len_emb = len_emb\n",
    "        self.mha1 = MultiHeadAttention(dim_kv, dim_q, len_emb, heads) #remove the masking from the attributes and add it to the call argument) \n",
    "        self.mha2 = MultiHeadAttention(dim_kv, dim_q, len_emb, heads) #same for here \n",
    "        self.drop = Dropout(rate = drop_rate)\n",
    "        self.layernorm = LayerNormalization(epsilon = epsilon)\n",
    "        self.dense =  FullFeedForward(dd_model, len_emb) \n",
    "        self.iter = iter\n",
    "\n",
    "\n",
    "#question! how does the built-in mha receive the number of q, k, v dims to map and create the q, k, v matrices? are the default. \n",
    "#question! during training will the layer normaliation parameters also train> if so, we need to define deperate layer norms to each. \n",
    "#question! there are some dense models in mha how are the number of neurons in them defined here? \n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, dec_pad_mask): \n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        for _ in range(iter):\n",
    "            \n",
    "            # Add positional Encoding: #remove the pos embeddings and have it in hte transformer. \n",
    "            #x += pos_emb(x.shape[1], self.len_emb)\n",
    "        \n",
    "            # Add a dropout layer: \n",
    "            x = self.drop(x, training = training) \n",
    "            \n",
    "            # Run through a MHA with the look-forward mask: \n",
    "            attn_mat1 = self.mha1(x, x, x, masking = look_ahead_mask)\n",
    "            \n",
    "            # Add dropout here during training:  \n",
    "            attn_mat1 = self.drop(attn_mat, training = training)\n",
    "            \n",
    "            # Add and Normalize: \n",
    "            attn_mat1_x = self.layernorm(attn_mat1 + x)\n",
    "            \n",
    "            # Run through the next MHA: \n",
    "            attn_mat2 = self.mha2(x , enc_output, enc_output, masking = dec_pad_mask)\n",
    "            \n",
    "            # Add dropout during training: \n",
    "            attn_mat2 = self.drop(attn_mat2, training = training) \n",
    "            \n",
    "            # Add and Normalize: \n",
    "            attn_mat2_x = self.layernorm(attn_mat2 +  attn_mat1_x) \n",
    "            \n",
    "            # Run through a dense layer: \n",
    "            dense_output = self.dense(attn_mat2_x)\n",
    "            \n",
    "            # Add Dropout: \n",
    "            dense_drop = self.drop(dense_output, training = training)\n",
    "            \n",
    "            # Add and Normalize: \n",
    "            x = self.layernorm(dense_drop + attn_mat2_x) #x is the res but remember that since it's in a loop we still call it x. \n",
    "            \n",
    "        return(x) \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2c66f01-2ae5-4ecb-99f0-9f4a7b885ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if it works after you've defined your output sequence (decoder input):  \n",
    "len_emb = 50 \n",
    "dim_kv = 30 \n",
    "dim_q = 50 \n",
    "heads = 3 \n",
    "dd_model = 20 \n",
    "iter = 3 \n",
    "drop_rate = 0.1\n",
    "function_decoder = Decoder(len_emb, dim_kv, dim_q, heads, \n",
    "                           dd_model, iter, drop_rate = 0.1, epsilon = 1e-6)\n",
    "\n",
    "function_decoder(y, output_encoder, training = True, look_ahead_mask = None, dec_pad_mask = None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2ac83-37ed-4dc2-8b76-48a34de41922",
   "metadata": {},
   "source": [
    "## Define the Transformer architecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53a127b8-7c92-423c-ad0d-68ea6ee5775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self, len_emb, dim_kv, dim_q, heads, d_model,\n",
    "                dd_model, iterEnc, iterDec, df_model, len_seq_out,\n",
    "                drop_rate = 0.1, epsilon = 1e-6):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        self.len_emb = len_emb\n",
    "        self.len_seq_out = len_seq_out\n",
    "        \n",
    "        self.encoder = Encoder(dim_kv, dim_q, heads, d_model, len_emb, iterEnc, drop_rate = 0.1)\n",
    "        \n",
    "        self.decoder = Decoder(len_emb, dim_kv, dim_q, heads, dd_model, iterDec, drop_rate = 0.1, epsilon = 1e-6)\n",
    "        \n",
    "        self.dense =  Dense(units = df_model,activation = 'softmax') \n",
    "        \n",
    "    def call(self, input_seqs, output_seqs, training, enc_pad_mask, dec_pad_mask, look_ahead_mask):\n",
    "    \n",
    "        \"\"\"\n",
    "        the output sequence and the input sequence must already be in the form of word embeddings added. we need two more paddings. <sos> and <eos> \n",
    "        len_seq in and out might be different \n",
    "        \"\"\"\n",
    "        \n",
    "        #first pass the input embeddings to add the positional encodings no dropouts necessary as the encoder already has it: \n",
    "        len_seq = input_seqs.shape[1]\n",
    "        input_seqs += pos_enc(len_seq_in, self.len_emb) \n",
    "        \n",
    "        #multiply by a constant for numerical stability #look into it! \n",
    "        input_seqs *= tf.math.sqrt(tf.cast(self.len_emb,tf.float32))\n",
    "        \n",
    "        # Run through the encoder part: \n",
    "        enc_output = self.encoder(input_seqs, training = training, masking = enc_pad_mask)\n",
    "        \n",
    "        # Add positional encoding for the output sequence: \n",
    "        output_seqs += pos_enc(self.len_seq_out, self.len_emb)\n",
    "        output_seqs *= tf.math.sqrt(tf.cast(self.len_emb,tf.float32))\n",
    "        \n",
    "        #Run through the decoder part: \n",
    "        dec_output = self.decoder(output_seqs, enc_output, training = training, look_ahead_mask = look_ahead_mask, dec_pad_mask = dec_pad_mask)\n",
    "        \n",
    "        # Run through a linear layer with activation function softmax \n",
    "        res = self.dense(dec_output) \n",
    "        return(res) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad6aed-290a-4b89-82f9-38c31f8f8e8b",
   "metadata": {},
   "source": [
    "before running through the final linear layer, do we add drop out to the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b3bc0-05c5-4c0a-8cf5-d934d3325778",
   "metadata": {},
   "source": [
    "For the word embeddings and if we are to use the decoder structure, we need to modify the word embeddings to also include two tokens : $<sos> $ start of the sentence and $<eos>$ end of the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49675bc1-db99-42df-953c-07b889b3bebe",
   "metadata": {},
   "source": [
    "We want the Softmax function that assigns the attention scores to avoid assigning any attention score to the padded parts of the sequence. So, instead we can either define a function that replaces vectors of all zeros with negative infinity (-1e-9) or when creating the padded embeddings for each input, we can assign -1e-9 to every padded token. But if we add the padding before going through the dot product attention (before the softmax), it is possible that through multiplication with matrices q,k, and v the padded vectors grow larger and then when we run the resultant matrix through softmax, it might again not assign 0 attention scores to the padded sequences. Therefore, the padded mask must be added after the dot product. Then apply Softhen multiply with the V matrix. Where to normalize? we will normalize the attention scores after the dot product before masking is applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff9997-82bd-4708-911a-fde51140d031",
   "metadata": {},
   "source": [
    "mPreferably, we want the input of the Encoder structure to already have the word embeddings and the positional encodings. In the Encoder structure, we will have the multi-head attention (think of it as running the self-attention multiple times) and a fully connected neural network which will be called FullFeedForward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27a212-c483-4c34-bf7e-437f2e5d3064",
   "metadata": {},
   "source": [
    "My intuition is that when the output is not normalized, the algo will be caught in many local minima or maxima and cannot easily and quickly converge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa067037-ec77-4047-ab47-cafa963125d6",
   "metadata": {},
   "source": [
    "change the layer norms as they are also trainable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5d26e-7464-4efe-8070-8d352f510595",
   "metadata": {},
   "source": [
    "# Questions\n",
    "Why is the embedding size also taken as an argument in MHA? we get matrices q, k, and v. The product of qTk will give a dim_k or dim_q by emb_size. The final product in the attention mechanism must yield a matrix of the same length of seq and emb_size. \n",
    "\n",
    "* look into the command of MHA.\n",
    "* LayerNormalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d3e37-1b9f-4d23-8aae-309e9146b3f6",
   "metadata": {},
   "source": [
    "### Multi-head attention? \n",
    "We will input 3 xs (possibly they could be different?) then the inputs are mapped linearly to give us the matrices Query, Key and Value. \n",
    "* dimension x (#batches, len_seq, len_emb)\n",
    "* dim of k:$K^T x$ if k is (len_seq,dim_k), then its transpose is (dim_k, len_seq), the resultant matrix is going to have dim (dim_k, len_emb)\n",
    "* dim of q: $Q^T x $; if q is (len_seq,dim_q), then its transpose is of dim (dim_q, len_emb) and the resultant dot product gives (dim_q,len_emb)\n",
    "* Similarly, for the multiplication of $V^T x$, we have the value being of dimension (dim_v, len_emb).\n",
    "  * if it is a self-attention (attention with only one head), then $qk^T$ has dim (dim_q, dim_k), scale, add the mask and dropout if given.\n",
    "  * if it has n heads, then we will produce query and key matrices of dimensions dim_q/n, dim_k/n. After the dot product, the result is of dim (dim_q/n, dim_k/n). We then concatenate these results to get the desired dim of (dim_q,dim_k). $ \\bold{make sure you understand the concatenation} $\n",
    "* dot prodcut v (dim_v, len_emb) qTk (dim_q, dim_k) --> $ qTk .v $ Note that here dim_k must be the same as the dimension of v for this dot product to occur.\n",
    "* just like magic, you have the attention scores now and the result is a matrix of (dim_k, len_emb).\n",
    "* so then we add our initial x and normalize too. in order to add x to the attention scores, the attention scores need to have the same dim as x. meaning that dim_k needs to be the same as the len of the sequence.\n",
    "\n",
    "### Fully Connected Neural Network: \n",
    "\n",
    "We feed the matrix out of the attention mechanism into the fully connected neural network. how many neurons? what matters is that the output layer must have len_emb neurons in order to match the dim of x. why do we need them to match? becoz we again add the input seq x to the result (after another layer of normalization). \n",
    "\n",
    "Then copy the result, pass as key and value to the decoder network. \n",
    "\n",
    "# Question isn't the dot product we are talking here actually a cross product?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2663a32-dd95-4d8a-aaf7-bd23f2a93b3c",
   "metadata": {},
   "source": [
    "Do you wanna define another function that takes the dims you'd like and deliver you the query, key and value matrices? \n",
    "because now we no longer need to have as inputs, the dim_kv and dim_q. would we need the masking? yes in self_attention. \n",
    "we need the mha to take 3 arguments as q,k,v. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c990e-9d1a-4269-9113-7bcc4e5d03d2",
   "metadata": {},
   "source": [
    "* How do we initialize the q, k, and v matrices?\n",
    "\n",
    "    A multi-head attention class is defined where based on the training x, created the q,k, and v matrices by applying a dense layer to the input sequence each time. \n",
    "\n",
    "\n",
    "* How is this model trained?\n",
    "  Still a question.\n",
    "\n",
    "* For the encoder layer, what attributes do we need?\n",
    "   * Better question to ask is what do we want the Encoder layer do?\n",
    "     When running the encoder layer, we want to input the input sequence; then this input sequence will go through to add word embeddings, then positional encodings. We then run the attention model on this to get the attention scores added to the structure. we then normalize and add dropout. Then run through a fully connected neural network, add x, normalize and add another dropout layer.\n",
    "\n",
    "* What is the purpose of the Dropout function and what are its arguments?\n",
    " \n",
    "  let's assume the dropout rate is 0.1. During training, the dropout layer randomly selects 10% of the input and replace it with zeros. This prevents the model to overfit the parameters based on the training set and also prevents the model to become too reliant on certain parameters. During the call function, make sure you set the training argument to 'True' so that the model will apply dropout only during training and does nothing during the inference mode (making predictions). \n",
    "\n",
    "* As an alternative to defining our own Multi-Head attention, we could use the one built-in Tensorflow package. Check out if the calculations are all the same and what the arguments to this layer are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eff716-87b5-4621-a661-7732206e6c9b",
   "metadata": {},
   "source": [
    "the next task is to have an encoder layer. you then have a decoder and then the transformer. to the transformer, we would like to only input the x and not modify to add embeddings or positional embeddings. but for the encoder part, we would like to repeat the encoder part multiple times. so essentially, we want to add a loop to the encoder section. how to do that? \n",
    "what is going to be on repeat? the full encoder layer.\n",
    "so what would be the input to the encoder? x \n",
    "at first, the x will be the training set but for the next iterations on the loop, we will take the output of the encoder and input for the next time. so, this in that sense it sequential but the length of the senquence is actually much less. I would like to see how would repeating the loop actually benefit training. \n",
    "* try adding multiple iterations of the encoder and then try with only one layer of encoder and see if there is a difference in the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d158f6f-fd4f-41f8-84b9-e89680434191",
   "metadata": {},
   "source": [
    "cool thing to know, you can use the underscore for any variable that is not gonna be used later. so for example, if you know a function will output 3 vars and you only need the first two, you can have the third variable saved as an underscore. or during a for loop, you can write for _ in range() this means that the place holder for the iterations will actually not be used inside the loop so you don't bother defining it. \n",
    "\n",
    "* Note that we must make sure in the attention paper bahdanua, we defined the correct variables to be saved and disregarded in the post-attention LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a001a10-6e74-4b6b-ac2b-dc82a70d6086",
   "metadata": {},
   "source": [
    "So what does the decoder do? \n",
    "it seems that the decoder but for the decoder to start we need the encoder code in coursera to be complete we then can move to it? not right now I am primed to work to have at least an understanding of the decoder before going through it we do not necessarily start the code right away. \n",
    "\n",
    "so what does a decoder do? the decoder, has also an input that is prob encoded with embeddings and the pos encodings. then the decoder must go through yet another mha. to this mha that takes 3 inputs, we input the query as the input of the decoder and we input the output of the encoding as the key and value. why? query is where the model is at prediction. so essentially, the query has info about what has already been predicted. then you pass on all the info about the input as the key so the model learns what part of the input to focus on most when making prediction at the next step. you then multiply the attention scores with the value matrix which is again the input encoded. so essentially, the decoder takes the info on what has already been predicted and the full key matrix (input encoded) decides which parts of the input to pay attention to the most and once the attention scores are calculated, then the attention scores are weigh the encoded input. this is beautiful! then the mha might repeat for several iterations and then the output is added and normalized to the initial input of the decoder. \n",
    "\n",
    "* the input of the decoder will go through a masked multi-head attention. might repeat multiple times. then you add the initial input embeddings and encoding to the output of the multi-head loop (after you add the dropout layer to it). then this is inputed into another mha as the query. the key and the value are taken as the output of the encoder. another mha in a loop. then you add the dropout layer and then add to the query of this mha. then normalize and then run through a ffn. then again add dropout and add the input of ffn to the output.\n",
    "\n",
    "there might be another linear map and the run through the softmax. and voila! \n",
    "\n",
    "ok so the first step is to modify our mha function. how? this model should take the query, key and values as inputs. previously, we would take the the input, and equal to the size of the input, we would calculate the query, key and value inside the mha. now take this calculation out. so the key, query and value will be defined outside the mha and inputed to reshape and cal attn scores. but note that this process must take place after the loop in the encoder is introduced. \n",
    "\n",
    "might also need to define a masked mha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620a428-a4a4-4ac8-b6ec-13f9ffd5c0cc",
   "metadata": {},
   "source": [
    "in case it was needed, we can run our x matrix in the jupyter notebook of coursera and check if the outputs and inputs are the same and if one model performs differently than the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed74b0-11b1-4f1f-99e2-c24334b9fdbc",
   "metadata": {},
   "source": [
    "# ? would this be helpful for the task of sentiment analysis? I believe it should be. \n",
    "\n",
    "in case it was needed, we can run our x matrix in the jupyter notebook of coursera and check if the outputs and inputs are the same and if one model performs differently than the other. \n",
    "#change the padding of all 1s to a padding of all zeros and see how the performance of the model might change. \n",
    "# you might also be interested in applying a padding to the model to examine the improvment in the performance. \n",
    "#need to add training = training for all the dropouts applied so this will only occur during the training mode. not that right now, the model is \n",
    "#always in the training mode. no inference so the dropout layer is also applied during inference. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1876d1-eb16-4451-b7d9-d2d5f4ebd6cb",
   "metadata": {},
   "source": [
    "There are multiple tasks that must be followed: \n",
    "1, build the decoder network from scratch. (today) \n",
    "2, build the transform's architecture (tom)\n",
    "3, learn about the dropouts (tom)\n",
    "4, learn about the masks (tom) \n",
    "5, apply the transformer to a task (2days each) 2 tasks (friday start this - sat done with one task) (sat - mon) finish the other task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517348fb-de0a-46dc-bc10-3bcedcf892ce",
   "metadata": {},
   "source": [
    "transformer: \n",
    "embeddings of the encoder and decoder should occur here but pos enc inside the encoder and decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec928f78-1c82-4858-a5c8-80b82d59be97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596ff51-b209-4b52-a910-b08c0eda1247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91892c6-90fa-48e6-9a7b-2e173e4b0a5c",
   "metadata": {},
   "source": [
    "The objective of this project is to: \n",
    "* Create positional encodings to capture sequential relationships in data\n",
    "* Calculate scaled dot-product self-attention with word embeddings\n",
    "* Implement masked multi-head attention\n",
    "* Build and train a Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf32da35-c73d-4fd9-ac13-b9611a826739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages: \n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization, Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import  reshape, shape, transpose\n",
    "\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9cb92-9293-4a30-b4cc-7c98a557daf1",
   "metadata": {},
   "source": [
    "We want the Softmax function that assigns the attention scores to avoid assigning any attention score to the padded parts of the sequence. So, instead we can either define a function that replaces vectors of all zeros with negative infinity (-1e-9) or when creating the padded embeddings for each input, we can assign -1e-9 to every padded token. But if we add the padding before going through the dot product attention (before the softmax), it is possible that through multiplication with matrices q,k, and v the padded vectors grow larger and then when we run the resultant matrix through softmax, it might again not assign 0 attention scores to the padded sequences. Therefore, the padded mask must be added after the dot product. Then apply Softmax, then multiply with the V matrix. Where to normalize? we will normalize the attention scores after the dot product before masking is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0640e104-b96c-4491-b450-dab4782f2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviewId            userName  \\\n",
      "0     gp:AOqpTOFxf3fttcT5DSvFIn9KPp5FErgH9yC533Fmoxv...      Donna Caritero   \n",
      "1     gp:AOqpTOEq6rNIWLnPV4KFTctWvm0mpGEQljtD6mvy1H-...  Soumi Mukhopadhyay   \n",
      "2     gp:AOqpTOE86hSyPRHZgYt28Uk5zGe4FZGb1hkmtFDiYJ2...   Theknown _unknown   \n",
      "3     gp:AOqpTOHSuKkVTcM3QgCCKysHQlxEnk2ocOKsUMiMIJy...        Anthony Dean   \n",
      "4     gp:AOqpTOEOrZt5H6jXPiplJyffCd5ZBnVXACTWgwNsF1R...   Neha Diana Wesley   \n",
      "...                                                 ...                 ...   \n",
      "1495  gp:AOqpTOHhnXMpylU3f-1V1KbR2hwWArOilxPlKI6K4xY...            Reen Ali   \n",
      "1496  gp:AOqpTOEcz62DHS-amqTB5xGMhM4_R0UJpcv_HDNny9i...     Shaurya Chilwal   \n",
      "1497  gp:AOqpTOFMqEqa_kpp29Q8wjcBmKUCAvOQGQx4KZQ8b83...           GK Gaming   \n",
      "1498  gp:AOqpTOGY4z3pUxeiqGzn2ad3Noxqlbm-9DZ3ksHqD1_...    1203_Vani Sharma   \n",
      "1499  gp:AOqpTOFVGZ0MXyR-Gv_d2cYf2KD709Hwple_u7OZE4y...           MeLLy EcK   \n",
      "\n",
      "                                              userImage  \\\n",
      "0     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "2     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "3     https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
      "4     https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "...                                                 ...   \n",
      "1495  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1496  https://play-lh.googleusercontent.com/a/AATXAJ...   \n",
      "1497  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1498  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "1499  https://play-lh.googleusercontent.com/a-/AOh14...   \n",
      "\n",
      "                                                 review  score  thumbsUpCount  \\\n",
      "0     Overall it's really an amazing app. I've been ...      4            528   \n",
      "1     Hey! Yes I gave a 5 star rating... coz I belie...      5            351   \n",
      "2     Canva used to be a good app! But recently I've...      1            160   \n",
      "3     It's a brilliant app, but I have just one prob...      5            145   \n",
      "4     This was such a great app. I used to make BTS ...      4            142   \n",
      "...                                                 ...    ...            ...   \n",
      "1495   Absolutely amazing and a lifesaver for teachers.      5              0   \n",
      "1496  Very nice but many a times shows error opening...      3              0   \n",
      "1497  Too much lag. Always stuck on starting page. W...      1              0   \n",
      "1498  Nice app for all college work. So many feature...      5              0   \n",
      "1499  I am a teacher and this was so much for this s...      5              0   \n",
      "\n",
      "     reviewCreatedVersion                  at  \\\n",
      "0                 2.116.0 2021-06-17 07:18:54   \n",
      "1                 2.116.0 2021-06-17 19:18:28   \n",
      "2                 2.116.0 2021-06-23 19:13:28   \n",
      "3                 2.116.0 2021-06-19 23:36:07   \n",
      "4                 2.116.0 2021-06-21 00:29:27   \n",
      "...                   ...                 ...   \n",
      "1495              2.127.0 2021-09-07 02:45:51   \n",
      "1496              2.123.0 2021-08-14 07:36:50   \n",
      "1497              2.118.0 2021-07-06 03:34:38   \n",
      "1498              2.123.0 2021-08-07 18:47:15   \n",
      "1499              2.117.0 2021-06-28 03:40:57   \n",
      "\n",
      "                                           replyContent           repliedAt  \\\n",
      "0     Hi Donna. We are sorry that your text or desig... 2021-06-19 21:24:32   \n",
      "1                                                   NaN                 NaT   \n",
      "2     Hi there. We're sorry to hear that you are hav... 2021-06-26 20:20:56   \n",
      "3                                                   NaN                 NaT   \n",
      "4     Hi Neha. Sorry to hear that you are unable to ... 2021-06-24 20:46:28   \n",
      "...                                                 ...                 ...   \n",
      "1495                                                NaN                 NaT   \n",
      "1496  Hi Shaurya, we're sorry if the app shows error... 2021-08-22 00:07:12   \n",
      "1497  Hello there, sorry if it is frustrating and ju... 2021-07-19 01:19:18   \n",
      "1498                                                NaN                 NaT   \n",
      "1499                                                NaN                 NaT   \n",
      "\n",
      "     Sentiment              Sub Category    Sub Category_test  \n",
      "0     Negative                       NaN  bug_picture_quality  \n",
      "1     Positive           extremely_happy                  NaN  \n",
      "2     Negative  saving_downloading_issue     bug_saving_files  \n",
      "3     Negative                    others        bug_app_crash  \n",
      "4     Negative                    others            bug_other  \n",
      "...        ...                       ...                  ...  \n",
      "1495  Positive           extremely_happy                  NaN  \n",
      "1496  Negative                    others                  NaN  \n",
      "1497  Negative                    others                  NaN  \n",
      "1498  Positive           extremely_happy                  NaN  \n",
      "1499  Positive           extremely_happy                  NaN  \n",
      "\n",
      "[1500 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#Loading the data: \n",
    "CustomerFeed = 'Canva_reviews.xlsx'\n",
    "df = pd.read_excel(CustomerFeed)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2c13f4-64e6-461e-ba0a-f47a02d1f2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overall it's really an amazing app. I've been ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey! Yes I gave a 5 star rating... coz I belie...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canva used to be a good app! But recently I've...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a brilliant app, but I have just one prob...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This was such a great app. I used to make BTS ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review Sentiment\n",
       "0  Overall it's really an amazing app. I've been ...  Negative\n",
       "1  Hey! Yes I gave a 5 star rating... coz I belie...  Positive\n",
       "2  Canva used to be a good app! But recently I've...  Negative\n",
       "3  It's a brilliant app, but I have just one prob...  Negative\n",
       "4  This was such a great app. I used to make BTS ...  Negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"review\", \"Sentiment\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efee0dca-791d-4461-87cc-a72c931a04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_txt(review):\n",
    "    \"\"\"\n",
    "    This function receives a text and returns it edited as follows: \n",
    "    1, all words converted to lower case \n",
    "    2, integers removed\n",
    "    3, tokenize the words \n",
    "    4, punctuation removed \n",
    "    5, common words that are unnecessary are removed. \n",
    "    \"\"\"\n",
    "    \n",
    "    review_edited = []\n",
    "\n",
    "    #Converting to lower case: \n",
    "    review_edited = review.lower() \n",
    "    \n",
    "    #Removing integers: \n",
    "    pattern = r'[0-9]'\n",
    "    # Match all digits in the string and replace them with an empty string\n",
    "    review_edited = re.sub(pattern, '', review_edited) \n",
    "\n",
    "    #Tokenize the comment: \n",
    "    review_edited = word_tokenize(review_edited) \n",
    "\n",
    "    #Removing punctuation \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    review_edited = [''.join(tokenizer.tokenize(word)) for word in review_edited if len(tokenizer.tokenize(word))>0]\n",
    "\n",
    "    #Removing common words: \n",
    "    remove_list = stopwords.words('english') \n",
    "    to_remove = [ \"not\",'don',\"don't\",'should',\"should've\", 'ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",'mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn', \"wouldn't\"]\n",
    " \n",
    "    review_edited = [word for word in review_edited if not word in remove_list]\n",
    "    return(review_edited) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc11020d-af99-4e71-98cd-1a53acc9df36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unable', 'save', 'work', 'nothing', 'works']\n",
      "Unable to save my work. Nothing works :(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Negative', 'Positive']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the reviews: \n",
    "x = df[\"review\"] \n",
    "\n",
    "#Modify the text to test the function reviews_edited: \n",
    "reviews_edited = [edit_txt(review) for review in x]\n",
    "print(reviews_edited[13])\n",
    "print(x[13])\n",
    "\n",
    "# Define the target dataset and extract the unique rankings: \n",
    "y = df[\"Sentiment\"].tolist()\n",
    "ranking = np.unique(y)\n",
    "ranking = ranking.tolist()\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659e7b7-d82a-465b-be12-cb4e9a83f09d",
   "metadata": {},
   "source": [
    "### <font color = \"red\"> Do we need the following? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d06fea0-e4bd-42f0-b9f0-724cbf2ee4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aap',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'acc',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accessibilities']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dictionary: \n",
    "Split = [] \n",
    "Dic = []\n",
    "dictionary = np.unique([word for review in reviews_edited for word in review]).tolist()\n",
    "\n",
    "# Add extra padding to limit the length of the input: \n",
    "dictionary = dictionary + [\"<pad>\"]\n",
    "dictionary[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d9e8f8-7b69-44e9-adac-e687dd985a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing datasets: \n",
    "#x = x.to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, \n",
    "                                   random_state=104,  \n",
    "                                   test_size=0.25,  \n",
    "                                   shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca92e7d-f6b9-4af3-99a0-c97369effba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the edit_txt function to both text corpus: \n",
    "X_train = [edit_txt(comment) for comment in X_train]\n",
    "X_test = [edit_txt(comment) for comment in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ce1576-ee25-4447-b1fd-bd3abe0ea73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word embeddings (Glove word embeddings) \n",
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "words =  list(embeddings_dict.keys())\n",
    "vectors = [embeddings_dict[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72957b17-242f-42a0-bdff-f77ef8329a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the input with Glove Word Embeddings: \n",
    "def gvec_input(x,m,e): \n",
    "    \"\"\"\n",
    "    \n",
    "    This function takes any input, x, and returns a glove vector based on the \n",
    "    words introduced in the vocabulary (400,000 words). This function returns k vectors where k is the number of words in the \n",
    "    sentence. Every vector corresponds to a word in the dictionary and each entry will describe a feature of the word. \n",
    "    \n",
    "    inputs: \n",
    "    \n",
    "    x (string) : a statement from customers. \n",
    "    m (int)    : size of the sequence \n",
    "    e (int)    : size of the embeddings \n",
    "    outputs: \n",
    "    v (m,n)    : where m is the number of words in the sentence and n = 50 is the number of total features describing a word. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    gv = np.zeros((n,m, e))\n",
    "    \n",
    "    for i in range(0, n): #looping over each comment \n",
    "        txt = x[i] #select the ith comment  \n",
    "        txt = (txt[:m] if len(txt) > m else txt + ['<pad>'] * (m - len(txt))) #shorten or add extra padding\n",
    "        for l in range(m): #looping over each word \n",
    "            \n",
    "            # add the embedding of all ones for pads\n",
    "            if txt[l] == \"<pad>\": \n",
    "                gv[i,l,:] = np.ones(e) \n",
    "                \n",
    "            # if a word is not is the list of Glove embeddings, then assign an array which is the average of all embeddings:    \n",
    "            elif txt[l] not in words: \n",
    "                gv[i,l,:] = np.mean(vectors, axis = 0)\n",
    "            # add the word embeddings: \n",
    "            else: \n",
    "                gv[i,l,:] = embeddings_dict[txt[l]]\n",
    "    return(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84cded1-8bc4-4eec-9f9b-cccb5b98983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the length of the sequence: \n",
    "m = 30 \n",
    "# The length of the embeddings: \n",
    "e = 50\n",
    "X_trainmod = gvec_input(X_train,m,e) \n",
    "X_testmod = gvec_input(X_test,m,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058fc745-0c8b-456d-8c03-f48b4cee67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79238999  0.21864     0.68711001 ... -0.066753   -0.39660001\n",
      "   0.74818999]\n",
      " [ 0.36998999  0.082841    0.16883001 ...  0.0053184  -0.50853002\n",
      "   0.24986   ]\n",
      " [ 0.02648     0.33737001  0.065667   ... -0.3398     -0.23043001\n",
      "   0.19069   ]\n",
      " ...\n",
      " [ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]\n",
      " [ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]\n",
      " [ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1125, 30, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_trainmod[0])\n",
    "X_trainmod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637d96b0-8376-4518-a333-fcb383b7670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the angles for positional embeddings: \n",
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get i from dimension span k\n",
    "    i = k//2\n",
    "    # Calculate the angles using pos, i and d\n",
    "    angles = pos/ (10000)**(2*i/d)\n",
    "\n",
    "    \n",
    "    return angles\n",
    "    \n",
    "def pos_emb(len_seq,len_emb): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates the positional embeddings for all the words in the sequence based on: \n",
    "    \n",
    "    Input: \n",
    "    len_seq (int) : The length of the sequences inputed into the model. \n",
    "    len_emb (int) : The length of the word embeddings for every word in the sequence. \n",
    "\n",
    "    Note: the size of the positional encoding and the word embeddings must match in order to add them in the next step. \n",
    "\n",
    "    Output: \n",
    "    res (np.array(len_seq, len_emb)) : ith row of this matrix represents the positional encodings for the ith position in the sequence. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    len_i = int(len_emb/2)\n",
    "\n",
    "    # Initialize the matrix to save positional encodings: \n",
    "    res = np.zeros((len_seq,len_emb))\n",
    "    angles = np.zeros((len_seq,len_emb))\n",
    "    \n",
    "    #for each position in the sequence \n",
    "    for pos in range(len_seq): \n",
    "        \n",
    "        #calculate the angles: \n",
    "        for i in range(len_i): \n",
    "            angles[pos,2*i] = pos/(10000**(2*i/len_emb))\n",
    "            angles[pos, 2*i +1] = pos/(10000**(2*i/len_emb)) \n",
    "        \n",
    "        # Calculate the entries corresponding to each position \n",
    "        #for j in range(len_i): \n",
    "        res[pos, 0::2] = np.sin(angles[pos,0::2])\n",
    "        res[pos,1::2] = np.cos(angles[pos,0::2])\n",
    "            \n",
    "    return(tf.cast(res.reshape(1,len_seq,len_emb), dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c529243c-143a-4c19-b33e-09f20339c327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 30, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the positional embeddings: \n",
    "position_enc = pos_emb(X_trainmod.shape[1],X_trainmod.shape[2])\n",
    "position_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "403a6f0c-6c2e-4f39-b203-df7d9ba0c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 30, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([375, 30, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the positional encoding to the word embeddings: \n",
    "X_trainmod = X_trainmod + position_enc \n",
    "print(X_trainmod.shape)\n",
    "\n",
    "X_testmod = X_testmod + position_enc \n",
    "X_testmod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2610eef7-b508-4824-986e-26fa7a3a42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(tensor): \n",
    "    \"\"\"\n",
    "    this function will encode the padded sequences as -1e-9 so that when they are run through a Softmax, the value will be equal to zero. \n",
    "    \"\"\"\n",
    "    # Identify rows where all elements are zero\n",
    "    is_zero_row = tf.reduce_all(tf.equal(tensor, 0), axis=1)\n",
    "\n",
    "    # Expand is_zero_row to match the shape of tensor\n",
    "    is_zero_row_expanded = tf.expand_dims(is_zero_row, axis=-1)\n",
    "\n",
    "    # Replace zeros with -1e-9 where the row is all zeros\n",
    "    result_tensor = tf.where(is_zero_row_expanded, tf.constant(-1e-9, dtype=tf.float64), tensor)\n",
    "    return(result_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cf1ef-cb5f-4700-a9cf-3c5bcb130d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now time to define the self_attention: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6376dbc-8cdc-4021-864d-00d204ab466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(q,k,v, masking):\n",
    "    \"\"\"\n",
    "    this function calculates a self_attention mechanism \n",
    "    res are the final attention scores. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Perform matrix multiplication on the last two dimensions\n",
    "    dotqk = tf.matmul(q, k, transpose_b = True)\n",
    "\n",
    "    dim_k = tf.cast(k.shape[-1],tf.float32)\n",
    "    normalized_dotqk = dotqk/tf.math.sqrt(dim_k)\n",
    "    \n",
    "    #then add the masking if masking if given\" \n",
    "    if masking is not None: \n",
    "        normalized_dotqk += (1 - masking)* (-1e9)\n",
    "    \n",
    "    attention_scores =  tf.nn.softmax(tf.cast(normalized_dotqk, dtype=tf.float32),axis = -1)\n",
    "    res = tf.matmul(attention_scores,v) \n",
    "    \n",
    "    return(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d24857a1-8baa-43b3-88ba-bf4af7b97f9c",
   "metadata": {},
   "source": [
    "def padding_mask(tensor): \n",
    "    \"\"\"\n",
    "    This function encodes the padded sequences as -1e-9 so that when they are run through a Softmax, the value will be equal to zero. \n",
    "    This version is adapted for a 2D tensor input.\n",
    "    \"\"\"\n",
    "    # Ensure the tensor has at least 2 dimensions\n",
    "    if tf.rank(tensor) != 2:\n",
    "        raise ValueError(\"Input tensor must be 2D with shape [sequence_length, embedding_size].\")\n",
    "    \n",
    "    # Identify rows where all elements are zero\n",
    "    is_zero_row = tf.reduce_all(tf.equal(tensor, 0), axis=1)\n",
    "\n",
    "    # Expand is_zero_row to match the shape of tensor\n",
    "    is_zero_row_expanded = tf.expand_dims(is_zero_row, axis=-1)\n",
    "\n",
    "    # Replace zeros with -1e-9 where the row is all zeros\n",
    "    result_tensor = tf.where(is_zero_row_expanded, tf.constant(-1e-9, dtype=tensor.dtype), tensor)\n",
    "    \n",
    "    return result_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbadeff-f5df-457a-8c51-2b22fba90fe7",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Review masked functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c10984-8a5d-4d05-9301-0fee3991b7f8",
   "metadata": {},
   "source": [
    "Preferably, we want the input of the Encoder structure to already have the word embeddings and the positional encodings. In the Encoder structure, we will have the multi-head attention (think of it as running the self-attention multiple times) and a fully connected neural network which will be called FullFeedForward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "645c8842-8c75-4d7d-84f6-7cc6dc1a2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullFeedForward(n_1, emb_size):#the model must return vectors of the same size as the embeddings of the input so can be combined with decoder\n",
    "    model = Sequential([\n",
    "    Dense(n_1, activation='tanh', name=\"dense1\"), #relu? (#samples, len_seq, n_1)\n",
    "    Dense(emb_size, activation='tanh', name=\"dense2\")# linear? (#samples, len_seq, emb_size)\n",
    "])\n",
    "    return(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e1408-13cd-4dec-a8dd-5b3a1aa5f512",
   "metadata": {},
   "source": [
    "# Questions\n",
    "Why is the embedding size also taken as an argument in MHA? we get matrices q, k, and v. The product of qTk will give a dim_k or dim_q by emb_size. The final product in the attention mechanism must yield a matrix of the same length of seq and emb_size. \n",
    "\n",
    "* look into the command of MHA.\n",
    "* LayerNormalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba488292-2fd2-4f80-9eaa-962064cb7ca3",
   "metadata": {},
   "source": [
    "### Multi-head attention? \n",
    "We will input 3 xs (possibly they could be different?) then the inputs are mapped linearly to give us the matrices Query, Key and Value. \n",
    "* dimension x (#batches, len_seq, len_emb)\n",
    "* dim of k:$K^T x$ if k is (len_seq,dim_k), then its transpose is (dim_k, len_seq), the resultant matrix is going to have dim (dim_k, len_emb)\n",
    "* dim of q: $Q^T x $; if q is (len_seq,dim_q), then its transpose is of dim (dim_q, len_emb) and the resultant dot product gives (dim_q,len_emb)\n",
    "* Similarly, for the multiplication of $V^T x$, we have the value being of dimension (dim_v, len_emb).\n",
    "  * if it is a self-attention (attention with only one head), then $qk^T$ has dim (dim_q, dim_k), scale, add the mask and dropout if given.\n",
    "  * if it has n heads, then we will produce query and key matrices of dimensions dim_q/n, dim_k/n. After the dot product, the result is of dim (dim_q/n, dim_k/n). We then concatenate these results to get the desired dim of (dim_q,dim_k). $ \\bold{make sure you understand the concatenation} $\n",
    "* dot prodcut v (dim_v, len_emb) qTk (dim_q, dim_k) --> $ qTk .v $ Note that here dim_k must be the same as the dimension of v for this dot product to occur.\n",
    "* just like magic, you have the attention scores now and the result is a matrix of (dim_k, len_emb).\n",
    "* so then we add our initial x and normalize too. in order to add x to the attention scores, the attention scores need to have the same dim as x. meaning that dim_k needs to be the same as the len of the sequence.\n",
    "\n",
    "### Fully Connected Neural Network: \n",
    "\n",
    "We feed the matrix out of the attention mechanism into the fully connected neural network. how many neurons? what matters is that the output layer must have len_emb neurons in order to match the dim of x. why do we need them to match? becoz we again add the input seq x to the result (after another layer of normalization). \n",
    "\n",
    "Then copy the result, pass as key and value to the decoder network. \n",
    "\n",
    "# Question isn't the dot product we are talking here actually a cross product?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1c8d12-2822-4f45-b2ff-0ffa7fa5e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_tensor(q_matrix, heads, pre_attention): \n",
    "    \n",
    "    #pre_attention, we'll need to reform into 4d \n",
    "    if pre_attention:\n",
    "\n",
    "        dense_qre = reshape(q_matrix, (shape(q_matrix)[0], shape(q_matrix)[1], heads, -1))\n",
    "        dense_qre = transpose(dense_qre, ([0, 2, 3, 1]))\n",
    "        \n",
    "        \n",
    "    #post_attention, we'll need to revert back to 3d: \n",
    "    else: \n",
    "        q_matrix_transpose = transpose(q_matrix, ([0,3,1,2]))\n",
    "        dense_qre = reshape(q_matrix_transpose, (shape(q_matrix_transpose)[0], shape(q_matrix_transpose)[1], -1)) \n",
    "        \n",
    "        \n",
    "    return(dense_qre)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91822-8143-466a-891c-ba004ea8b6db",
   "metadata": {},
   "source": [
    "Do you wanna define another function that takes the dims you'd like and deliver you the query, key and value matrices? \n",
    "because now we no longer need to have as inputs, the dim_kv and dim_q. would we need the masking? yes in self_attention. \n",
    "we need the mha to take 3 arguments as q,k,v. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400abad-4aea-4e45-a883-4088db864b62",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MultiHeadAttentionM(Layer):  # Ensure this name matches in `super()`\n",
    "    def __init__(self, len_emb, heads, masking, query_mat, \n",
    "                 key_mat, value_mat, **kwargs):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)  # Ensure the class name here is correct\n",
    "        self.heads = heads\n",
    "        self.masking = masking\n",
    "        self.d_model = len_emb\n",
    "        self.mha = self_attention()\n",
    "        self.query = query_mat \n",
    "        self.key = key_mat\n",
    "        elf.value = value_mat \n",
    "\n",
    "    \n",
    "    def call(self,x,**kwargs): #by passing self, you passed all the attributes you've defined above. \n",
    "     \n",
    "        # Reshape: \n",
    "        dense_qre = reshape_tensor(self.query, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        dense_kre = reshape_tensor(self.key, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_k/heads, len_seq)\n",
    "        dense_vre = reshape_tensor(self.value, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_v/heads, len_seq) \n",
    "        \n",
    "        # Calculate the attention scores: \n",
    "        attention_scores = self.mha(dense_qre, dense_kre,dense_vre, masking) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        \n",
    "        # Revert the shape:\n",
    "        attention_with_v = reshape_tensor(attention_scores, self.heads, pre_attention = False) #shape = (#samples, len_seq, dim_q)\n",
    "        \n",
    "        # Run through another dense and add to the initial x: \n",
    "        res = Dense(units = self.d_model)(attention_with_v)  # shape = (#samples, len_seq, d_model) \n",
    "        #how to add the dropout and the normalization layers? \n",
    "        return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1380463-0062-4718-b56c-dd21b0823aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MultiHeadAttention(Layer):  # Ensure this name matches in `super()`\n",
    "    def __init__(self, dim_kv, dim_q, len_emb, heads, **kwargs):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)  # Ensure the class name here is correct\n",
    "        self.dim_k = self.dim_v = dim_kv\n",
    "        self.dim_q = dim_q\n",
    "        self.heads = heads\n",
    "        self.d_model = len_emb\n",
    "\n",
    "    \n",
    "    def call(self,q,k,v, masking = None, **kwargs): #by passing self, you passed all the attributes you've defined above. \n",
    "       \n",
    "        # Define the query, key, and value matrices: \n",
    "        dense_q = Dense(units = self.dim_q)(q) # shape = (#samples, len_seq, dim_q)\n",
    "        dense_k = Dense(units = self.dim_k)(k) # shape = (#samples, len_seq, dim_k) \n",
    "        dense_v = Dense(units = self.dim_v)(v) # shape = (#samples, len_seq, dim_v) \n",
    "        \n",
    "        # Reshape: \n",
    "        dense_qre = reshape_tensor(dense_q, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        dense_kre = reshape_tensor(dense_k, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_k/heads, len_seq)\n",
    "        dense_vre = reshape_tensor(dense_v, self.heads, pre_attention = True) #shape = (#samples, #heads, dim_v/heads, len_seq) \n",
    "        \n",
    "        # Calculate the attention scores: \n",
    "        attention_scores = self_attention(dense_qre, dense_kre,dense_vre, masking) #shape = (#samples, #heads, dim_q/heads, len_seq)\n",
    "        \n",
    "        # Revert the shape:\n",
    "        attention_with_v = reshape_tensor(attention_scores, self.heads, pre_attention = False) #shape = (#samples, len_seq, dim_q)\n",
    "        \n",
    "        # Run through another dense and add to the initial x: \n",
    "        res = Dense(units = self.d_model)(attention_with_v)  # shape = (#samples, len_seq, d_model) \n",
    "        \n",
    "        return(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63ba52-8a09-41f6-8c68-f4d408b6435c",
   "metadata": {},
   "source": [
    "how to add the dropout and the normalization layers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ce61987-9ca6-4042-ae9d-846c54c1199b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1125, 30, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_kv = 30 \n",
    "dim_q = 20 \n",
    "len_emb = 50\n",
    "heads = 2 \n",
    "\n",
    "masking = None\n",
    "\n",
    "function = MultiHeadAttention(dim_kv, dim_q, len_emb, heads)\n",
    "function(X_trainmod, X_trainmod,X_trainmod, masking = None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4ff0c-c0d8-46a9-bf21-8628e681814d",
   "metadata": {},
   "source": [
    "* How do we initialize the q, k, and v matrices?\n",
    "\n",
    "    A multi-head attention class is defined where based on the training x, created the q,k, and v matrices by applying a dense layer to the input sequence each time. \n",
    "\n",
    "\n",
    "* How is this model trained?\n",
    "  Still a question.\n",
    "\n",
    "* For the encoder layer, what attributes do we need?\n",
    "   * Better question to ask is what do we want the Encoder layer do?\n",
    "     When running the encoder layer, we want to input the input sequence; then this input sequence will go through to add word embeddings, then positional encodings. We then run the attention model on this to get the attention scores added to the structure. we then normalize and add dropout. Then run through a fully connected neural network, add x, normalize and add another dropout layer.\n",
    "\n",
    "* What is the purpose of the Dropout function and what are its arguments?\n",
    " \n",
    "  let's assume the dropout rate is 0.1. During training, the dropout layer randomly selects 10% of the input and replace it with zeros. This prevents the model to overfit the parameters based on the training set and also prevents the model to become too reliant on certain parameters. During the call function, make sure you set the training argument to 'True' so that the model will apply dropout only during training and does nothing during the inference mode (making predictions). \n",
    "\n",
    "* As an alternative to defining our own Multi-Head attention, we could use the one built-in Tensorflow package. Check out if the calculations are all the same and what the arguments to this layer are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40656c8b-23c9-403e-9bc8-566f49c50af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    \n",
    "    def __init__(self, dim_kv, dim_q, heads, fnn_neurons, len_emb, drop_rate, iter):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        self.mha     = MultiHeadAttention(dim_kv, dim_q, len_emb, heads)\n",
    "        self.norm    = LayerNormalization(epsilon = 1e-6)\n",
    "        self.drop    = Dropout(rate = drop_rate)\n",
    "        self.fnn     = FullFeedForward(fnn_neurons, len_emb)\n",
    "        self.iter    = iter\n",
    "\n",
    "        \n",
    "    def call(self,x,training, masking): \n",
    "        \n",
    "        \n",
    "        for _ in range(self.iter): \n",
    "\n",
    "            # Add dropout layer: \n",
    "            drop_x = self.drop(x, training = training) \n",
    "            \n",
    "            # Calculate the attention scores: \n",
    "            mha_scores = self.mha(drop_x, drop_x, drop_x, masking = masking)\n",
    "        \n",
    "            # Add dropout and normalize: \n",
    "            dropout_1 = self.drop(mha_scores, training = training)\n",
    "            norm_1  = self.norm(dropout_1 + x )\n",
    "        \n",
    "            #Run through a fully connected neural network: \n",
    "            fnn_output = self.fnn(norm_1) \n",
    "            \n",
    "            # Add dropout: \n",
    "            dropout_2 = self.drop(fnn_output, training = training)\n",
    "        \n",
    "            # Normalize: \n",
    "            x = self.norm(dropout_2 + norm_1)\n",
    "            \n",
    "        return x\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "289dfe62-5023-400c-9778-08acedc534a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1125, 30, 50])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_kv = 30 \n",
    "dim_q = 20 \n",
    "len_emb = 50\n",
    "heads = 2 \n",
    "masking = None \n",
    "fnn_neurons = 20\n",
    "drop_rate = 0.1\n",
    "function = Encoder(dim_kv, dim_q, heads, fnn_neurons, len_emb, drop_rate,10)\n",
    "output_encoder = function(X_trainmod, masking = None)\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca7202-9866-4316-8700-273063bda28d",
   "metadata": {},
   "source": [
    "the next task is to have an encoder layer. you then have a decoder and then the transformer. to the transformer, we would like to only input the x and not modify to add embeddings or positional embeddings. but for the encoder part, we would like to repeat the encoder part multiple times. so essentially, we want to add a loop to the encoder section. how to do that? \n",
    "what is going to be on repeat? the full encoder layer.\n",
    "so what would be the input to the encoder? x \n",
    "at first, the x will be the training set but for the next iterations on the loop, we will take the output of the encoder and input for the next time. so, this in that sense it sequential but the length of the senquence is actually much less. I would like to see how would repeating the loop actually benefit training. \n",
    "* try adding multiple iterations of the encoder and then try with only one layer of encoder and see if there is a difference in the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f342003-c087-40ac-bedc-0c90013d7a33",
   "metadata": {},
   "source": [
    "cool thing to know, you can use the underscore for any variable that is not gonna be used later. so for example, if you know a function will output 3 vars and you only need the first two, you can have the third variable saved as an underscore. or during a for loop, you can write for _ in range() this means that the place holder for the iterations will actually not be used inside the loop so you don't bother defining it. \n",
    "\n",
    "* Note that we must make sure in the attention paper bahdanua, we defined the correct variables to be saved and disregarded in the post-attention LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526dce7-92f4-4f41-873b-1ff92ead5aa2",
   "metadata": {},
   "source": [
    "So what does the decoder do? \n",
    "it seems that the decoder but for the decoder to start we need the encoder code in coursera to be complete we then can move to it? not right now I am primed to work to have at least an understanding of the decoder before going through it we do not necessarily start the code right away. \n",
    "\n",
    "so what does a decoder do? the decoder, has also an input that is prob encoded with embeddings and the pos encodings. then the decoder must go through yet another mha. to this mha that takes 3 inputs, we input the query as the input of the decoder and we input the output of the encoding as the key and value. why? query is where the model is at prediction. so essentially, the query has info about what has already been predicted. then you pass on all the info about the input as the key so the model learns what part of the input to focus on most when making prediction at the next step. you then multiply the attention scores with the value matrix which is again the input encoded. so essentially, the decoder takes the info on what has already been predicted and the full key matrix (input encoded) decides which parts of the input to pay attention to the most and once the attention scores are calculated, then the attention scores are weigh the encoded input. this is beautiful! then the mha might repeat for several iterations and then the output is added and normalized to the initial input of the decoder. \n",
    "\n",
    "* the input of the decoder will go through a masked multi-head attention. might repeat multiple times. then you add the initial input embeddings and encoding to the output of the multi-head loop (after you add the dropout layer to it). then this is inputed into another mha as the query. the key and the value are taken as the output of the encoder. another mha in a loop. then you add the dropout layer and then add to the query of this mha. then normalize and then run through a ffn. then again add dropout and add the input of ffn to the output.\n",
    "\n",
    "there might be another linear map and the run through the softmax. and voila! \n",
    "\n",
    "ok so the first step is to modify our mha function. how? this model should take the query, key and values as inputs. previously, we would take the the input, and equal to the size of the input, we would calculate the query, key and value inside the mha. now take this calculation out. so the key, query and value will be defined outside the mha and inputed to reshape and cal attn scores. but note that this process must take place after the loop in the encoder is introduced. \n",
    "\n",
    "might also need to define a masked mha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1ae65-b1d8-4dd8-815f-4eeeb683d275",
   "metadata": {},
   "source": [
    "in case it was needed, we can run our x matrix in the jupyter notebook of coursera and check if the outputs and inputs are the same and if one model performs differently than the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465fe18-9e79-4792-b6d3-1b08487d6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#come back to the training which we have set to true for all code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe40d6-1c74-471e-bd0e-914365959944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? would this be helpful for the task of sentiment analysis? I believe it should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21020813-3ebe-4b40-be49-9c48f9f6fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the padding of all 1s to a padding of all zeros and see how the performance of the model might change. \n",
    "# you might also be interested in applying a padding to the model to examine the improvment in the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddfd23-3ba1-4267-99b5-79f9a3d0e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add training = training for all the dropouts applied so this will only occur during the training mode. not that right now, the model is \n",
    "#always in the training mode. no inference so the dropout layer is also applied during inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e5f21-f3b1-4018-849d-8288983c6a87",
   "metadata": {},
   "source": [
    "There are multiple tasks that must be followed: \n",
    "1, build the decoder network from scratch. (today) \n",
    "2, build the transform's architecture (tom)\n",
    "3, learn about the dropouts (tom)\n",
    "4, learn about the masks (tom) \n",
    "5, apply the transformer to a task (2days each) 2 tasks (friday start this - sat done with one task) (sat - mon) finish the other task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c75cbb94-77cc-4907-a737-fd8d5d907a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self, len_emb, dim_kv, dim_q, heads, \n",
    "                dd_model, iter, \n",
    "                drop_rate = 0.1, epsilon = 1e-6):  #dd_model is the number of neurons in the last layer of decoder (dense with softmax) \n",
    "        super(Decoder, self).__init__()\n",
    "        self.len_emb = len_emb\n",
    "        self.mha1 = MultiHeadAttention(dim_kv, dim_q, len_emb, heads) #remove the masking from the attributes and add it to the call argument) \n",
    "        self.mha2 = MultiHeadAttention(dim_kv, dim_q, len_emb, heads) #same for here \n",
    "        self.drop = Dropout(rate = drop_rate)\n",
    "        self.layernorm = LayerNormalization(epsilon = epsilon)\n",
    "        self.dense =  FullFeedForward(dd_model, len_emb) \n",
    "        self.iter = iter\n",
    "\n",
    "\n",
    "#question! how does the built-in mha receive the number of q, k, v dims to map and create the q, k, v matrices? are the default. \n",
    "#question! during training will the layer normaliation parameters also train> if so, we need to define deperate layer norms to each. \n",
    "#question! there are some dense models in mha how are the number of neurons in them defined here? \n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, dec_pad_mask): \n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        for _ in range(iter):\n",
    "            \n",
    "            # Add positional Encoding: #remove the pos embeddings and have it in hte transformer. \n",
    "            #x += pos_emb(x.shape[1], self.len_emb)\n",
    "        \n",
    "            # Add a dropout layer: \n",
    "            x = self.drop(x, training = training) \n",
    "            \n",
    "            # Run through a MHA with the look-forward mask: \n",
    "            attn_mat1 = self.mha1(x, x, x, masking = look_ahead_mask)\n",
    "            \n",
    "            # Add dropout here during training:  \n",
    "            attn_mat1 = self.drop(attn_mat, training = training)\n",
    "            \n",
    "            # Add and Normalize: \n",
    "            attn_mat1_x = self.layernorm(attn_mat1 + x)\n",
    "            \n",
    "            # Run through the next MHA: \n",
    "            attn_mat2 = self.mha2(x , enc_output, enc_output, masking = dec_pad_mask)\n",
    "            \n",
    "            # Add dropout during training: \n",
    "            attn_mat2 = self.drop(attn_mat2, training = training) \n",
    "            \n",
    "            # Add and Normalize: \n",
    "            attn_mat2_x = self.layernorm(attn_mat2 +  attn_mat1_x) \n",
    "            \n",
    "            # Run through a dense layer: \n",
    "            dense_output = self.dense(attn_mat2_x)\n",
    "            \n",
    "            # Add Dropout: \n",
    "            dense_drop = self.drop(dense_output, training = training)\n",
    "            \n",
    "            # Add and Normalize: \n",
    "            x = self.layernorm(dense_drop + attn_mat2_x) #x is the res but remember that since it's in a loop we still call it x. \n",
    "            \n",
    "        return(x) \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2c66f01-2ae5-4ecb-99f0-9f4a7b885ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_emb = 50 \n",
    "dim_kv = 30 \n",
    "dim_q = 50 \n",
    "heads = 3 \n",
    "dd_model = 20 \n",
    "iter = 3 \n",
    "drop_rate = 0.1\n",
    "function_decoder = Decoder(len_emb, dim_kv, dim_q, heads, \n",
    "                           dd_model, iter, drop_rate = 0.1, epsilon = 1e-6)\n",
    "# Let's create an input to the decoder: note that the inputs now are difference and so are their embeddings. \n",
    "y  = tf.zeros((1,1,1))\n",
    "#? won't know until I run an example for the decoder network. \n",
    "#function_decoder(y, output_encoder, training = True, look_ahead_mask = None, dec_pad_mask = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734820dd-6e20-4fa1-b25a-c20a9aa8bfae",
   "metadata": {},
   "source": [
    "transformer: \n",
    "embeddings of the encoder and decoder should occur here but pos enc inside the encoder and decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a127b8-7c92-423c-ad0d-68ea6ee5775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self, len_emb, dim_kv, dim_q, heads, d_model\n",
    "                dd_model, iterEnc, iterDec, df_model, len_seq_out,\n",
    "                drop_rate = 0.1, epsilon = 1e-6):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        self.len_emb = len_emb\n",
    "        self.len_seq_out = len_seq_out\n",
    "        \n",
    "        self.encoder = Encoder(dim_kv, dim_q, heads, d_model, len_emb, drop_rate = 0.1, iterEnc)\n",
    "        \n",
    "        self.decoder = Decoder(len_emb, dim_kv, dim_q, heads, dd_model, iterDec, drop_rate = 0.1, epsilon = 1e-6)\n",
    "        \n",
    "        self.dense =  Dense(units = df_model,activation = 'softmax') \n",
    "        \n",
    "    def call(self, input_seqs, output_seqs, training, enc_pad_mask, dec_pad_mask, look_ahead_mask)\n",
    "    \n",
    "        \"\"\"\n",
    "        the output sequence and the input sequence must already be in the form of word embeddings added. we need two more paddings. <sos> and <eos> \n",
    "        len_seq in and out might be different \n",
    "        \"\"\"\n",
    "        \n",
    "    #first pass the input embeddings to add the positional encodings no dropouts necessary as the encoder already has it: \n",
    "    len_seq = input_seqs.shape[1]\n",
    "    input_seqs += pos_enc(len_seq_in, self.len_emb) \n",
    "    \n",
    "    #multiply by a constant for numerical stability #look into it! \n",
    "    input_seqs *= tf.math.sqrt(tf.cast(self.len_emb,tf.float32))\n",
    "    \n",
    "    # Run through the encoder part: \n",
    "    enc_output = self.encoder(input_seqs, training = training, masking = enc_pad_mask)\n",
    "    \n",
    "    # Add positional encoding for the output sequence: \n",
    "    output_seqs += pos_enc(self.len_seq_out, self.len_emb)\n",
    "    output_seqs *= tf.math.sqrt(tf.cast(self.len_emb,tf.float32))\n",
    "    \n",
    "    #Run through the decoder part: \n",
    "    dec_output = self.decoder(output_seqs, enc_output, training = training, look_ahead_mask = look_ahead_mask, dec_pad_mask = dec_pad_mask)\n",
    "    \n",
    "    # Run through a linear layer with activation function softmax \n",
    "    res = self.dense(dec_output) \n",
    "    return(re\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad6aed-290a-4b89-82f9-38c31f8f8e8b",
   "metadata": {},
   "source": [
    "before running through the final linear layer, do we add drop out to the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ec1c1-3e8f-4ac4-bda1-b0627002e3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05c982-2eb9-4a6b-8b84-49b7321ecc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb7119-f21d-4d72-adc7-8fbaeb1abe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd27a212-c483-4c34-bf7e-437f2e5d3064",
   "metadata": {},
   "source": [
    "My intuition is that when the output is not normalized, the algo will be caught in many local minima or maxima and cannot easily and quickly converge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa067037-ec77-4047-ab47-cafa963125d6",
   "metadata": {},
   "source": [
    "change the layer norms as they are also trainable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9e895-e86d-407e-8c8c-ea7c85d83e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9678cc-2b85-4654-9aa5-4fe96993e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8488cf-25ac-4a3a-9a81-21f60ef83a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5602fc-fea1-4eb4-a771-d44f0120c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

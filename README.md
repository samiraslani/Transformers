# Transformers
 This is Transformer model with a dot product attention built from scratch.

 The file [Dot_product_Attention](Dot_product_Attention/) contains a general code for a transformer's architecture developed from scratch. 

 The transformer's code is then used for different prediction and language generation tasks which are mentioned below: 
 * Sentiment Analysis: Several approaches for this task are already implemented from applying simple RNNs (accuracy did not surpass 70%) to bi-directional encoder decoder LSTMs with attention (accuracy reached 96%). This time, another attention mechanism will be applied to this task based on the paper "Attention is All you Need."
 * Named-entity Recognition:










# Add references. 

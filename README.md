# Transformers
 In this project, we aim to build a transformer's architecture from scratch. The file [Dot_product_Attention](Dot_product_Attention/) contains a general code for a transformer's architecture and is based on the paper [Attention is All you Need.](https://arxiv.org/abs/1706.03762)
 
 The transformer's code is then implemented for various tasks as the following: 
 
 * Sentiment Analysis: Several approaches for this task are already implemented from applying simple RNNs to bi-directional encoder decoder LSTMs equipped with [additive attention(https://github.com/samiraslani/LSTM-model). The task of sentiment analysis is also examined with a transformer encoder model. The results can be found [here](Sentiment-DotAtten.ipynb/)

 * Named-entity Recognition:










# Add references. 

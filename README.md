# Transformers
 In this project, we aim to build a transformer's architecture from scratch. The file [Dot_product_Attention](/Dot_product_Attention.ipynb
/) contains a general code for a transformer's architecture and is based on the paper [Attention is All you Need.](https://arxiv.org/abs/1706.03762)
 
 The transformer's code is then implemented for various tasks as the following: 
 
 * **Sentiment Analysis**:
   
   Several approaches for this task are already implemented from applying simple RNNs to bi-directional encoder decoder LSTMs equipped with [additive attention](https://github.com/samiraslani/Additive-Attention-model). The task of sentiment analysis is also examined with a transformer encoder model. The results can be found [here](Sentiment-DotAtten.ipynb/)

 * **Named-entity Recognition**:
   will use pre-trained models from Hugging Face library
   then fine tune 










# Add references. 
